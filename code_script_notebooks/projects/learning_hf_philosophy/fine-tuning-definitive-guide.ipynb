{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5385487,"sourceType":"datasetVersion","datasetId":3122881},{"sourceId":163950613,"sourceType":"kernelVersion"},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup the environment\n!pip install --upgrade huggingface_hub >> /dev/null\n!pip install git+https://github.com/huggingface/transformers -U accelerate peft >> /dev/null\n!pip install -i https://pypi.org/simple/ bitsandbytes >> /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-23T08:06:27.464013Z","iopub.execute_input":"2024-02-23T08:06:27.464875Z","iopub.status.idle":"2024-02-23T08:07:25.890191Z","shell.execute_reply.started":"2024-02-23T08:06:27.464836Z","shell.execute_reply":"2024-02-23T08:07:25.889165Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-ns1b7f8s\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\ntransformers.__version__","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:25.892469Z","iopub.execute_input":"2024-02-23T08:07:25.892798Z","iopub.status.idle":"2024-02-23T08:07:25.899368Z","shell.execute_reply.started":"2024-02-23T08:07:25.892770Z","shell.execute_reply":"2024-02-23T08:07:25.898496Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'4.39.0.dev0'"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nimport torch\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:25.900483Z","iopub.execute_input":"2024-02-23T08:07:25.900804Z","iopub.status.idle":"2024-02-23T08:07:25.910850Z","shell.execute_reply.started":"2024-02-23T08:07:25.900780Z","shell.execute_reply":"2024-02-23T08:07:25.909988Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Load the model\nmodel_id = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:25.912984Z","iopub.execute_input":"2024-02-23T08:07:25.913289Z","iopub.status.idle":"2024-02-23T08:07:26.187161Z","shell.execute_reply.started":"2024-02-23T08:07:25.913265Z","shell.execute_reply":"2024-02-23T08:07:26.186340Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_id,\n                                            device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:26.188278Z","iopub.execute_input":"2024-02-23T08:07:26.188556Z","iopub.status.idle":"2024-02-23T08:07:27.714299Z","shell.execute_reply.started":"2024-02-23T08:07:26.188523Z","shell.execute_reply":"2024-02-23T08:07:27.711559Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3434\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   3430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3432\u001b[0m     )\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3434\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3442\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m get_max_memory(max_memory)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:910\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[1;32m    909\u001b[0m user_not_set_max_memory \u001b[38;5;241m=\u001b[39m max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[1;32m    913\u001b[0m     num_devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m max_memory \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(d)\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m max_memory[d] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:780\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[0;32m--> 780\u001b[0m             _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m         max_memory \u001b[38;5;241m=\u001b[39m {i: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmem_get_info(i)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())}\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# allocate everything in the mps device as the RAM is shared\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.715402Z","iopub.status.idle":"2024-02-23T08:07:27.715902Z","shell.execute_reply.started":"2024-02-23T08:07:27.715660Z","shell.execute_reply":"2024-02-23T08:07:27.715683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.717626Z","iopub.status.idle":"2024-02-23T08:07:27.718066Z","shell.execute_reply.started":"2024-02-23T08:07:27.717838Z","shell.execute_reply":"2024-02-23T08:07:27.717857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_prompt(prompt, your_model):\n    input_ids = tokenizer(prompt, return_tensors='pt')\n    logits = your_model.generate(**input_ids.to(device),\n                           max_new_tokens=50)\n    return tokenizer.decode(logits[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.719605Z","iopub.status.idle":"2024-02-23T08:07:27.720034Z","shell.execute_reply.started":"2024-02-23T08:07:27.719815Z","shell.execute_reply":"2024-02-23T08:07:27.719833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.max_len_single_sentence","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.721503Z","iopub.status.idle":"2024-02-23T08:07:27.721882Z","shell.execute_reply.started":"2024-02-23T08:07:27.721720Z","shell.execute_reply":"2024-02-23T08:07:27.721735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_test = \"Give me the top 15 golf equipment company names.\"\n\norginal_model_out = test_prompt(text_test, model)\n\norginal_model_out","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.722972Z","iopub.status.idle":"2024-02-23T08:07:27.723291Z","shell.execute_reply.started":"2024-02-23T08:07:27.723134Z","shell.execute_reply":"2024-02-23T08:07:27.723148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import load_dataset, load_metric, Dataset\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.724750Z","iopub.status.idle":"2024-02-23T08:07:27.725084Z","shell.execute_reply.started":"2024-02-23T08:07:27.724936Z","shell.execute_reply":"2024-02-23T08:07:27.724949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ndata = []\n\nwith open('/kaggle/input/databricks-dolly-15k/databricks-dolly-15k.jsonl') as file:\n    for line in file:\n        features = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if features[\"context\"]:\n            continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        data.append(template.format(**features))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.726454Z","iopub.status.idle":"2024-02-23T08:07:27.726835Z","shell.execute_reply.started":"2024-02-23T08:07:27.726658Z","shell.execute_reply":"2024-02-23T08:07:27.726679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_500 = [x for x in data if len(x) <= 500]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.727961Z","iopub.status.idle":"2024-02-23T08:07:27.728303Z","shell.execute_reply.started":"2024-02-23T08:07:27.728143Z","shell.execute_reply":"2024-02-23T08:07:27.728158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_500)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.729369Z","iopub.status.idle":"2024-02-23T08:07:27.729723Z","shell.execute_reply.started":"2024-02-23T08:07:27.729544Z","shell.execute_reply":"2024-02-23T08:07:27.729561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_500[10:25]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.730708Z","iopub.status.idle":"2024-02-23T08:07:27.731063Z","shell.execute_reply.started":"2024-02-23T08:07:27.730872Z","shell.execute_reply":"2024-02-23T08:07:27.730886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take 1500 from that data \ndata = data_500[:2500]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.732754Z","iopub.status.idle":"2024-02-23T08:07:27.733085Z","shell.execute_reply.started":"2024-02-23T08:07:27.732927Z","shell.execute_reply":"2024-02-23T08:07:27.732942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer(data[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.734408Z","iopub.status.idle":"2024-02-23T08:07:27.734776Z","shell.execute_reply.started":"2024-02-23T08:07:27.734619Z","shell.execute_reply":"2024-02-23T08:07:27.734634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json_ds = Dataset.from_pandas(pd.DataFrame(data=data))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.735903Z","iopub.status.idle":"2024-02-23T08:07:27.736228Z","shell.execute_reply.started":"2024-02-23T08:07:27.736069Z","shell.execute_reply":"2024-02-23T08:07:27.736084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_instruction(row):\n    # print(row)\n    input_token = tokenizer(row['0'], padding=\"max_length\",\n                            truncation=True, max_length=512)\n    input_token['labels'] = input_token['input_ids'].copy()\n    return input_token","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.738069Z","iopub.status.idle":"2024-02-23T08:07:27.738391Z","shell.execute_reply.started":"2024-02-23T08:07:27.738234Z","shell.execute_reply":"2024-02-23T08:07:27.738248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_tokenized = json_ds.map(tokenize_instruction,\n                                remove_columns=[\"0\"],\n                                batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.740124Z","iopub.status.idle":"2024-02-23T08:07:27.740584Z","shell.execute_reply.started":"2024-02-23T08:07:27.740343Z","shell.execute_reply":"2024-02-23T08:07:27.740363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_ds = dataset_tokenized.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.741872Z","iopub.status.idle":"2024-02-23T08:07:27.742316Z","shell.execute_reply.started":"2024-02-23T08:07:27.742081Z","shell.execute_reply":"2024-02-23T08:07:27.742100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_ds","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.743747Z","iopub.status.idle":"2024-02-23T08:07:27.744191Z","shell.execute_reply.started":"2024-02-23T08:07:27.743954Z","shell.execute_reply":"2024-02-23T08:07:27.743973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(process_ds['train'][1850]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.745846Z","iopub.status.idle":"2024-02-23T08:07:27.746284Z","shell.execute_reply.started":"2024-02-23T08:07:27.746053Z","shell.execute_reply":"2024-02-23T08:07:27.746071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_memory_footprint() / (1024 * 1024 * 1024)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.747341Z","iopub.status.idle":"2024-02-23T08:07:27.747777Z","shell.execute_reply.started":"2024-02-23T08:07:27.747556Z","shell.execute_reply":"2024-02-23T08:07:27.747575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch=len(process_ds[\"train\"])//(8*1)\n\nsteps_per_epoch","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.749083Z","iopub.status.idle":"2024-02-23T08:07:27.749542Z","shell.execute_reply.started":"2024-02-23T08:07:27.749296Z","shell.execute_reply":"2024-02-23T08:07:27.749314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"gpt_qlora_minimal\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"steps\",\n    logging_steps=1,\n    eval_steps=steps_per_epoch,\t\t# eval and save once per epoch  \t\n    save_strategy=\"epoch\",\n    num_train_epochs=5,\n    lr_scheduler_type=\"constant\",\n    # optim=\"paged_adamw_32bit\",\n    learning_rate=0.0002,\n    group_by_length=True,\n    # fp16=True,\n    # ddp_find_unused_parameters=False,\n    push_to_hub=False,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.750716Z","iopub.status.idle":"2024-02-23T08:07:27.751141Z","shell.execute_reply.started":"2024-02-23T08:07:27.750919Z","shell.execute_reply":"2024-02-23T08:07:27.750938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datacollator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n                                              mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.752180Z","iopub.status.idle":"2024-02-23T08:07:27.752559Z","shell.execute_reply.started":"2024-02-23T08:07:27.752341Z","shell.execute_reply":"2024-02-23T08:07:27.752355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=datacollator,\n    train_dataset=process_ds[\"train\"],\n    eval_dataset=process_ds[\"test\"],\n    args=args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.754853Z","iopub.status.idle":"2024-02-23T08:07:27.755167Z","shell.execute_reply.started":"2024-02-23T08:07:27.755008Z","shell.execute_reply":"2024-02-23T08:07:27.755021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# process_ds['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.756309Z","iopub.status.idle":"2024-02-23T08:07:27.756625Z","shell.execute_reply.started":"2024-02-23T08:07:27.756460Z","shell.execute_reply":"2024-02-23T08:07:27.756472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.757782Z","iopub.status.idle":"2024-02-23T08:07:27.758079Z","shell.execute_reply.started":"2024-02-23T08:07:27.757931Z","shell.execute_reply":"2024-02-23T08:07:27.757944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_test = \"What is Sunshine Recession?\"\n\norginal_model_out = test_prompt(text_test, model)\n\ntrained_model_out = test_prompt(text_test, merged_model)\n\nprint(\"Base model\")\nprint(original_model_out)\n\nprint(\"FT model\")\nprint(trained_model_outed)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T08:07:27.758996Z","iopub.status.idle":"2024-02-23T08:07:27.759306Z","shell.execute_reply.started":"2024-02-23T08:07:27.759149Z","shell.execute_reply":"2024-02-23T08:07:27.759169Z"},"trusted":true},"execution_count":null,"outputs":[]}]}