{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Prompting helps guide language model behavior by adding some input text specific to a task. \n\nPrompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. \n\nThis way, you can use one pretrained model whose weights are frozen, and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate mode","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install peft >> /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:52:00.527630Z","iopub.execute_input":"2024-02-04T14:52:00.528018Z","iopub.status.idle":"2024-02-04T14:52:18.786434Z","shell.execute_reply.started":"2024-02-04T14:52:00.527988Z","shell.execute_reply":"2024-02-04T14:52:18.784827Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    default_data_collator,\n    get_linear_schedule_with_warmup\n)\nfrom peft import (\n    get_peft_config,\n    get_peft_model,\n    PromptTuningInit,\n    PromptTuningConfig,\n    TaskType,\n    PeftType\n)\nimport torch\n\nfrom datasets import load_dataset\nimport os\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndevice = \"cuda\"\nmodel_name_or_path = \"bigscience/bloomz-560m\"\ntokenizer_name_or_path = \"bigscience/bloomz-560m\"\n\n# This looks like a more interesting way of instructing\n# llm to get what we are looking for.\npeft_config = PromptTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n    prompt_tuning_init=PromptTuningInit.TEXT,\n    num_virtual_tokens=8,\n    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n    tokenizer_name_or_path=model_name_or_path,\n)\n\ndataset_name = \"twitter_complaints\"\n\ncheckpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n    \"/\", \"_\"\n)\n\ntext_column = \"Tweet text\"\nlabel_column = \"text_label\"\n\nmax_length = 64\nlr = 3e-2\n\nnum_epochs = 50\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:53:39.579380Z","iopub.execute_input":"2024-02-04T14:53:39.579847Z","iopub.status.idle":"2024-02-04T14:54:03.974912Z","shell.execute_reply.started":"2024-02-04T14:53:39.579811Z","shell.execute_reply":"2024-02-04T14:54:03.973923Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-04 14:53:49.497340: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-04 14:53:49.497500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-04 14:53:49.687239: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(\"ought/raft\",\n                       dataset_name)\n\ndataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:54:06.278046Z","iopub.execute_input":"2024-02-04T14:54:06.278835Z","iopub.status.idle":"2024-02-04T14:54:17.889149Z","shell.execute_reply.started":"2024-02-04T14:54:06.278798Z","shell.execute_reply":"2024-02-04T14:54:17.887994Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e84c0a2a894af9bd3e86847736d1dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/56.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08bcc8a43ebe4fc79d99931d5aa4625a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset raft/twitter_complaints (download: 9.30 MiB, generated: 366.13 KiB, post-processed: Unknown size, total: 9.66 MiB) to /root/.cache/huggingface/datasets/ought___raft/twitter_complaints/1.1.0/79c4de1312c1e3730043f7db07179c914f48403101f7124e2fe336f6f54d9f84...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75d149e2bf654c8ab31b3a805af59da6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f16b40e427411dbad3d58d182e3c60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/662k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b39bccc6907483ab110df21c398afc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7a1409f19040888e3a2264868818cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/327k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2298d260a2b74b25b688b312ac4741cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"465ccac43b9c49aa8e8f096914e3c452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/917k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d841facf6394801b1677d86acf22051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/54.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d637cf3a354951932c0e40185b1492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecde6111622f488d9bfeab9a37e272c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/70.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcdc6ad670be447d940d32ebc9cd91c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/196k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd418b68ba84f4e834b47b198d6cdcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afcbf6bacc0449d2a826146a67fcc22f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/412k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe8069677604f50b2267ba2b6abcc11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/52.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4846189430fa452993697111b22eed81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c677c8ef298449ec9fa5340930f2db3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/201k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ae7b15f1b6428a92fb039857e33b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aa3ef37f0b9455dad0f982f32bd906e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.64k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1a9a9f16a94a5e970cac7d11784fa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/412k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e5fa8029ed543ce96edb5200358ed34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5db6e45bb946c49be4fcec8fd8d14b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/336k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258ab04e40ec4b66b900705a9f4afb02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c844124df48f440aaadc44a294f75f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/68.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f5c6f0ffa74266925c34483c12171a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e754563eb68245299c238c16bbf8214d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3399 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset raft downloaded and prepared to /root/.cache/huggingface/datasets/ought___raft/twitter_complaints/1.1.0/79c4de1312c1e3730043f7db07179c914f48403101f7124e2fe336f6f54d9f84. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9f36988ee9245828a43eb19175729bd"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'Tweet text': '@HMRCcustomers No this is my first job', 'ID': 0, 'Label': 2}"},"metadata":{}}]},{"cell_type":"code","source":"classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n\ndataset = dataset.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n    batched=True,\n    num_proc=1,\n)\n\ndataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:54:47.140680Z","iopub.execute_input":"2024-02-04T14:54:47.141117Z","iopub.status.idle":"2024-02-04T14:54:47.227229Z","shell.execute_reply.started":"2024-02-04T14:54:47.141084Z","shell.execute_reply":"2024-02-04T14:54:47.226115Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92a8ae288924d0e9839251382fb6dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcbe4c4bbbe24dbfb4d56d9fd8487d09"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'Tweet text': '@HMRCcustomers No this is my first job',\n 'ID': 0,\n 'Label': 2,\n 'text_label': 'no complaint'}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\ntarget_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) \\\n                         for class_label in classes])\n\nprint(target_max_length)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:55:13.680381Z","iopub.execute_input":"2024-02-04T14:55:13.680812Z","iopub.status.idle":"2024-02-04T14:55:15.943753Z","shell.execute_reply.started":"2024-02-04T14:55:13.680779Z","shell.execute_reply":"2024-02-04T14:55:15.942846Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304397046a5941aca1bdeb1626f31bbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed1eb0ed2cc406796f6f3260556e920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d1dd7a696fe46a4b58f1caaf4bb6ee7"}},"metadata":{}},{"name":"stdout","text":"3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create a preprocess_function to:\n\n- Tokenize the input text and labels.\n\n- For each example in a batch, pad the labels with the tokenizers pad_token_id.\n\n- Concatenate the input text and labels into the model_inputs.\n\n- Create a separate attention mask for labels and model_inputs.\n\n- Loop through each example in the batch again to pad the input ids, labels, and attention mask to the max_length and convert them to PyTorch tensors.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    batch_size = len(examples[text_column])\n    \n    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n    targets = [str(x) for x in examples[label_column]]\n    model_inputs = tokenizer(inputs)\n    \n    labels = tokenizer(targets)\n    \n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n        # print(i, sample_input_ids, label_input_ids)\n        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n    \n    # print(model_inputs)\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n            max_length - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n            \"attention_mask\"\n        ][i]\n        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"test\"]\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    collate_fn=default_data_collator,\n    batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_dataset,\n                             collate_fn=default_data_collator,\n                             batch_size=batch_size,\n                             pin_memory=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nprint(model.print_trainable_parameters())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model_id = \"your-name/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\"\nmodel.push_to_hub(\"your-name/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\",\n                  use_auth_token=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_id = \"stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\"\n\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(model, peft_model_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(\n    f'{text_column} : {\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n    return_tensors=\"pt\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\n\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n    )\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n","metadata":{},"execution_count":null,"outputs":[]}]}