{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft py7zr >> /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import (\n    load_dataset,\n    concatenate_datasets\n)\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n)\n\ndataset = load_dataset('samsum')\n\nmodel_id = \"google/flan-t5-xxl\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:18:49.956640Z","iopub.execute_input":"2024-02-05T15:18:49.957051Z","iopub.status.idle":"2024-02-05T15:18:59.994395Z","shell.execute_reply.started":"2024-02-05T15:18:49.957014Z","shell.execute_reply":"2024-02-05T15:18:59.992812Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5612f44b67846968ae0f0125bcee48b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"106e14979e6849569fc8a3907ec6dc83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d94402c9fc6401f9407bce41aec1ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed918e990a34e0ab25c69e6acb1629c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09de093ff32741f0a3829bed148fcf3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fb6cb49cb1b49daad31b74508a1a550"}},"metadata":{}}]},{"cell_type":"code","source":"# Abstractive Summarization is a text-generation task\nimport numpy as np\nconcated_ds = concatenate_datasets([dataset['train'],\n                                        dataset['test']])","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:26:52.743974Z","iopub.execute_input":"2024-02-05T15:26:52.744426Z","iopub.status.idle":"2024-02-05T15:26:52.757130Z","shell.execute_reply.started":"2024-02-05T15:26:52.744391Z","shell.execute_reply":"2024-02-05T15:26:52.755667Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1317: FutureWarning: promote has been superseded by promote_options='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenized_dialog(row):\n    return tokenizer(row[\"dialogue\"],\n              truncation=True)\n    \ndef tokenized_summary(row):\n    return tokenizer(row[\"summary\"],\n              truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:33:21.382053Z","iopub.execute_input":"2024-02-05T15:33:21.382578Z","iopub.status.idle":"2024-02-05T15:33:21.390621Z","shell.execute_reply.started":"2024-02-05T15:33:21.382539Z","shell.execute_reply":"2024-02-05T15:33:21.388852Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = concated_ds.map(tokenized_dialog,\n                                   batched=True,\n                                   num_proc=4,\n                                   remove_columns=['id','dialogue','summary'])\n\ntokenized_outputs = concated_ds.map(tokenized_summary,\n                                    batched=True,\n                                    num_proc=4,\n                                    remove_columns=['id','dialogue','summary'])","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:33:23.394614Z","iopub.execute_input":"2024-02-05T15:33:23.395151Z","iopub.status.idle":"2024-02-05T15:33:28.545205Z","shell.execute_reply.started":"2024-02-05T15:33:23.395111Z","shell.execute_reply":"2024-02-05T15:33:28.543548Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"    ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c7f20b41fc406d9785bd415dc92599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87e7c4f21184129bffbf09377bc9f8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41188167ef204613a7a06148118f92e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f504bd332bad454081c74eba7c8d4182"}},"metadata":{}},{"name":"stdout","text":"    ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1317: FutureWarning: promote has been superseded by promote_options='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1283: FutureWarning: promote has been superseded by promote_options='default'.\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cab97a44a0f44b6db2c1b8ce82bad83c"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fbb1edec03a4c0eaa483c7c0a067b87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ef0c8ed82a435fbd41b63d0116c11d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688cf35c77314c2aa0ba8f58f6615498"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:33:31.922019Z","iopub.execute_input":"2024-02-05T15:33:31.922497Z","iopub.status.idle":"2024-02-05T15:33:31.933379Z","shell.execute_reply.started":"2024-02-05T15:33:31.922457Z","shell.execute_reply":"2024-02-05T15:33:31.931223Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 15551\n})"},"metadata":{}}]},{"cell_type":"code","source":"inp_length = [len(x) for x in tokenized_inputs['input_ids']]\nmax_source_length = int(np.percentile(inp_length, 85))\noutput_length = [len(x) for x in tokenized_outputs['input_ids']]\nmax_tgt_length = int(np.percentile(output_length, 90))","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:33:34.944375Z","iopub.execute_input":"2024-02-05T15:33:34.944827Z","iopub.status.idle":"2024-02-05T15:33:37.358475Z","shell.execute_reply.started":"2024-02-05T15:33:34.944794Z","shell.execute_reply":"2024-02-05T15:33:37.356685Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"a = np.array([[10, 7, 4], [3, 2, 1]])\n\nnp.percentile(a, 50)\n\n3.5\n\nnp.percentile(a, 50, axis=0)\n\narray([6.5, 4.5, 2.5])\n\nnp.percentile(a, 50, axis=1)\n\narray([7.,  2.])\n\nnp.percentile(a, 50, axis=1, keepdims=True)\n\narray([[7.],","metadata":{}},{"cell_type":"code","source":"print(max_source_length)\nprint(max_tgt_length)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:33:56.800957Z","iopub.execute_input":"2024-02-05T15:33:56.801507Z","iopub.status.idle":"2024-02-05T15:33:56.808126Z","shell.execute_reply.started":"2024-02-05T15:33:56.801452Z","shell.execute_reply":"2024-02-05T15:33:56.806806Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"255\n50\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.pad_token","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:46:41.981828Z","iopub.execute_input":"2024-02-05T15:46:41.982349Z","iopub.status.idle":"2024-02-05T15:46:41.992375Z","shell.execute_reply.started":"2024-02-05T15:46:41.982294Z","shell.execute_reply":"2024-02-05T15:46:41.990312Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'<pad>'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:46:56.102541Z","iopub.execute_input":"2024-02-05T15:46:56.103592Z","iopub.status.idle":"2024-02-05T15:46:56.110742Z","shell.execute_reply.started":"2024-02-05T15:46:56.103548Z","shell.execute_reply":"2024-02-05T15:46:56.109415Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_dataset(sample, padding='max_length'):\n    # assuming, the data is going to be batched\n    inputs = [f\"summarise {item}\" for item in sample[\"dialogue\"]]\n    \n    model_ins = tokenizer(inputs,\n                          max_length=max_source_length,\n                         padding=padding,\n                         truncation=True)\n    labels = tokenizer(sample['summary'],\n                       max_length=max_tgt_length,\n                      padding=padding,\n                      truncation=True)\n    \n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ] # if it is not pad_token_id then leave it as it is, else replace with -100\\\n    \n    model_ins['labels'] = labels[\"input_ids\"]\n    \n    return model_ins","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:50:37.245891Z","iopub.execute_input":"2024-02-05T15:50:37.246387Z","iopub.status.idle":"2024-02-05T15:50:37.256863Z","shell.execute_reply.started":"2024-02-05T15:50:37.246352Z","shell.execute_reply":"2024-02-05T15:50:37.255411Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"lm_das = dataset.map(preprocess_dataset,\n                    batched=True,\n                    num_proc=6,\n                    remove_columns=[\"id\", \"dialogue\",\n                                   \"summary\"])\nlm_das","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:50:39.466929Z","iopub.execute_input":"2024-02-05T15:50:39.467318Z","iopub.status.idle":"2024-02-05T15:50:46.987527Z","shell.execute_reply.started":"2024-02-05T15:50:39.467289Z","shell.execute_reply":"2024-02-05T15:50:46.985972Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"          ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159a6faa525e4aafa7e54986a099d148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb983523780b4cecb9f76ac000616158"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5695b0c0686840ec88ef2b419697946a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#4:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c3413ec34746599ab26e264c0f6035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb59fb68bbb4125bab95f4c99b2ece2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#5:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90145d9a0a14c528ef64ddeae887740"}},"metadata":{}},{"name":"stdout","text":"      ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1317: FutureWarning: promote has been superseded by promote_options='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n","output_type":"stream"},{"name":"stdout","text":"    ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe35c85a1ec4c959b4a1ded666298a5"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94dc4fd0cada42fc85cbe23fb110099b"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b5bda9dc63a41cd972908c6b1737eb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#5:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"810ca1c18c2c40ddae3993419aeed424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86cfb75d2e554f1ca1a3fb7406b5ea39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#4:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818f6a00be784e30adfa36f3d645fb9d"}},"metadata":{}},{"name":"stdout","text":"         ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eb4b8cbe0514258869ce385403cdd77"}},"metadata":{}},{"name":"stdout","text":"   ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc22ad21d66420cad25b33b01832354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca5b3c23c014260925ce474df47e12f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f48b61cdf342b1a3acb1ca0c89a626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#4:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c846c16dc99c46f5b083f2b6b90a8c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#5:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2207bda385146529915ffd528657869"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id,\n                                             load_in_8bit=True,\n                                             device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T15:53:52.420913Z","iopub.execute_input":"2024-02-05T15:53:52.421373Z","iopub.status.idle":"2024-02-05T15:53:55.839360Z","shell.execute_reply.started":"2024-02-05T15:53:52.421311Z","shell.execute_reply":"2024-02-05T15:53:55.837055Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db5697c024841ed87a2bb803f0b68f9"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSeq2SeqLM\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3032\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m-> 3032\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m   3034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3035\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `pip install bitsandbytes`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3038\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."],"ename":"RuntimeError","evalue":"No GPU found. A GPU is needed for quantization.","output_type":"error"}]},{"cell_type":"code","source":"from peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType,\n    prepare_model_for_kbit_training,\n    prepare_model_for_int8_training\n)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nint_model = prepare_model_for_int8_training(model)\n\nqt_model = get_peft_model(int_model, lora_config) \n\nqt_model.print_trainable_parameters()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\noutput_dir=\"lora-flan-t5-xxl\"\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n\tauto_find_batch_size=True,\n    learning_rate=1e-3, # higher learning rate\n    num_train_epochs=5,\n    logging_dir=f\"{output_dir}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    save_strategy=\"no\",\n    report_to=\"tensorboard\",\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n# trainer.model.base_model.save_pretrained(peft_model_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"local_path = \"~/training_models/lora_t5_samsum_training\"\npeft_model_id=\"results\"\ntrainer.model.save_pretrained(local_path)\ntokenizer.save_pretrained(local_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"results\"\nconfig = PeftConfig.from_pretrained(local_path)\n\n# load base LLM model and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,\n                                              load_in_8bit=True,\n                                              device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model,\n                                  local_path,\n                                  device_map={\"\":0})\nmodel.eval()\n\nprint(\"Peft model loaded\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"samsum\")\nsample = dataset['test'][randrange(len(dataset[\"test\"]))]\n\ninput_ids = tokenizer(sample[\"dialogue\"],\n                      return_tensors=\"pt\",\n                      truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids,\n                         max_new_tokens=10,\n                         do_sample=True,\n                         top_p=0.9)\n\nprint(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n\nprint(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kommonly used metrics to evaluate summarization task is rogue_score short for Recall-Oriented Understudy for Gisting Evaluation","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\ndef evaluate_peft_model(sample,max_target_length=50):\n    # generate summary\n    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(),\n                             do_sample=True,\n                             top_p=0.9,\n                             max_new_tokens=max_target_length)\n    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(),\n                                  skip_special_tokens=True)\n    # decode eval sample\n    # Replace -100 in the labels as we can't decode them.\n    \n    labels = np.where(sample['labels'] != -100,\n                      sample['labels'],\n                      tokenizer.pad_token_id)\n    \n    labels = tokenizer.decode(labels,\n                              skip_special_tokens=True)\n\n    # Some simple post-processing\n    return prediction, labels\n\n# load test dataset from distk\ntest_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n\n# run predictions\n# this can take ~45 minutes\npredictions, references = [] , []\nfor sample in tqdm(test_dataset):\n    p,l = evaluate_peft_model(sample)\n    predictions.append(p)\n    references.append(l)\n\n# compute metric\nrogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n\n# print results\nprint(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\nprint(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\nprint(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\nprint(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}