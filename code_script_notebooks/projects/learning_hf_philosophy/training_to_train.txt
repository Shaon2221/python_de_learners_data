Steps to Train any model in Hugging Face: Reliably

- Bringing in the Datasets
    > Review the datasets, its features and names
    If you are loading from csv, json or parquet ensure the 
    columns are clean, and you know the column names
    > Need to work on the data, based on the task at hand. 
    (Need to complete other tutorial NBs in HF) 
    > Load the dataset based on the splits
    > Create a Dataloader, Iterator out of the dataset
- Bringing in the Tokenisers
    > Decide on the type of tokenizer that best suits
    > Practice creating new tokenizers and training them using own corpus
    > Setup the function that tokenizes and returns the ids
        + Review the padding, max_length, truncate options, review output
- Preprocessing functions:
    > Tokenise the input sequences, and remove the text data 
    > To process the input_ids for the task, write/ import the 
    functions, depending on the task 
    > Map the imported functions on the dataset, 
- Setup Training:  
    > Instantiate the Training Arguments
    > Instantiate DataCollators if required
    > Instantiate the post-processing collator to support trainer
    > Build the Trainer, with datasets and collators. 
    > Start the training
- Work on Post Processing:
    > Instantiate the metrics
    > Write post-processing function for evaluation
    > Run the evaluation and get the resuls

- Reference data locations
Transformers Universe:
https://huggingface.co/spaces/Kamaljp/transformers_universe
Inference Demo:
https://github.com/insightbuilder/python_de_learners_data/blob/main/code_script_notebooks/projects/learning_hf_philosophy/inference_peft.ipynb
FineTuningGPT:
https://github.com/insightbuilder/python_de_learners_data/blob/main/code_script_notebooks/projects/learning_hf_philosophy/fine-tuning-definitive-guide.ipynb
Dolly_DB_datasets: (Thanks to Databricks)
https://github.com/insightbuilder/python_de_learners_data/blob/main/code_script_notebooks/projects/learning_hf_philosophy/databricks-dolly-15k.jsonl
Testing Google Gemma:
https://www.kaggle.com/code/kamaljp/testing-gemma/notebook
Tokenizer Training:
https://github.com/insightbuilder/python_de_learners_data/blob/main/code_script_notebooks/projects/learning_hf_philosophy/training-tf-tokenizers.ipynb
Transformers Documentation Tutorials:
https://huggingface.co/docs/transformers/en/notebooks#pytorch-nlp

chapters of (Steps to Master Fine Tuning LLMs To Ultimate AI Proficiency : A Definitive Guide)
0:00 Overview Intro
0:30 Handson Overview
2:25 GPU NVdia-smi Explanation 
4:30 Inference with Gemma Model
6:16 Preparing to Load Gemma Model
11:20 Unloading model from Memory 
12:45 Tokenizer Load & Intro
15:10 Locating Model Files on HardDisk
18:30 Tokeniser Config explanation
19:40 Model Config explanation
22:10 Loading model to GPU 
23:40 Loading model to RAM
24:50 Moving Model to GPU
27:50 Explaining Inference Process
29:00 Inferencing & VRAM Usage
30:50 Inferencing Flow Chart Explanation
32:40 PEFT Fine-tuned model Explanation 
36:35 PEFT Model Loading & OOM 
37:25 Quantising & Loading PEFT Model
39:50 Inferencing PEFT Model
41:00 Interim Recap
44:00 Finetuning GPT2 Flow Chart Explanation
46:30 Training process Explanation
49:40 Loading GPT2 Tokenizer & Model
52:30 Introducing Dolly Dataset
54:00 What is Causal LM 
56:00 Tokenizer Training Intro
59:30 Data Exploration & Tokenising
1:02:00 Padding Truncating with Tokeniser 
1:03:50 Transformers Universe Space Intro
1:04:50 Tokenizer Option Demo
1:07:30 Input Ids, Attention Masks & Labels 
1:11:50 Why Padding / Truncating Input
1:13:10 Training Arguments Explained
1:16:20 Datacollator & Trainer args
1:17:00 Training & Memory usage
1:18:10 Demo FTuned Model
1:20:00 Why FTuned model is not Good
1:21:40 Using Smaller Batch & Save VRAM
1:22:50 Recap
1:23:35 Outro 

Quantising LLM & FTing
0:00 Intro
0:52 Video Overview
2:00 Intro To Jupyter Environment
2:50 Imports Categories
5:50 Introducing Model Trained
7:20 Loading Raw Mistral Model
8:25 Unloading Mistral Model from GPU
11:40 Loading & Unloading BloomZ Model
14:05 Why Quantisation & Configs
17:25 Purpose of notes in Comments
18:35 Quantising to 8-bit model
21:15 Trying to Train Quantised Model
23:50 Model Architectures Intro
26:50 Interim Recap
28:20 LoRa Intro & NLP resources
32:00 LoRa Config Parameter
35:10 PEFT Prompt Methods
36:05 IA3 Method
37:50 Quantising Model to 4-Bit
42:10 Do Not Prep Model For Training
46:30 Failed Trainer with 4-bit Model
47:45 Modifying bf16 to True
49:00 Sharing Added Tricks
52:00 Recap
53:10 Outro
