Steps to Train any model in Hugging Face: Reliably

- Bringing in the Datasets
    > Review the datasets, its features and names
    If you are loading from csv, json or parquet ensure the 
    columns are clean, and you know the column names
    > Need to work on the data, based on the task at hand. 
    (Need to complete other tutorial NBs in HF) 
    > Create a Dataloader, Iterator out of the dataset
- Bringing in the Tokenisers
    > Decide on the type of tokenizer that best suits
    > Practice creating new tokenizers and training them using own corpus
    > Setup the function that tokenizes and returns the ids
- Preprocessing functions:
    > Tokenise the input sequences, and remove the text data 
    > To process the input_ids for the task, write/ import the 
    functions, depending on the task 
    > Map the imported functions on the dataset, 
- Setup Training:  
    > Instantiate the Training Arguments
    > Instantiate DataCollators if required
    > Build the Trainer, with datasets and collators. 
    > Start the training
