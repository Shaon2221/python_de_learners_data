{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer\nfb_model = \"facebook/blenderbot-400M-distill\"  #\ntokenizer = AutoTokenizer.from_pretrained(fb_model)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T13:36:06.393466Z","iopub.execute_input":"2024-02-06T13:36:06.394135Z","iopub.status.idle":"2024-02-06T13:36:15.877698Z","shell.execute_reply.started":"2024-02-06T13:36:06.394094Z","shell.execute_reply":"2024-02-06T13:36:15.876476Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2df7fd704aa4621b8060ccdabab3552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd6fc3f02ee42149e9b4e15ab7f5682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/127k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aceb2a4035604106b6858efee7f164af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/62.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a8dd20623b43cb87c9e1e81323b126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa63976f3b0842bdbcdcfe09b24f8ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e36a9067ad0a4bcdb0d79bdc9069c598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/310k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ff941abfbd4d88838ab1aa57fbfe20"}},"metadata":{}}]},{"cell_type":"code","source":"chat = [\n    {\"role\": \"user\",\"content\": \"Hello, How are you\"},\n    {\"role\": \"assistant\", \"content\": \"I am doing good. How can I help you\"},\n    {\"role\": \"user\", \"content\": \"How can I show off chat template\"}\n]","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:38:19.025676Z","iopub.execute_input":"2024-02-06T13:38:19.026690Z","iopub.status.idle":"2024-02-06T13:38:19.030810Z","shell.execute_reply.started":"2024-02-06T13:38:19.026652Z","shell.execute_reply":"2024-02-06T13:38:19.029939Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer.apply_chat_template(chat, tokenize=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:38:52.166302Z","iopub.execute_input":"2024-02-06T13:38:52.166724Z","iopub.status.idle":"2024-02-06T13:38:52.217149Z","shell.execute_reply.started":"2024-02-06T13:38:52.166695Z","shell.execute_reply":"2024-02-06T13:38:52.215758Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\nNo chat template is defined for this tokenizer - using the default template for the BlenderbotTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"' Hello, How are you  I am doing good. How can I help you   How can I show off chat template</s>'"},"metadata":{}}]},{"cell_type":"code","source":"mistral_model = \"mistralai/Mistral-7B-Instruct-v0.1\"  # 750 MB model\ntokenizer = AutoTokenizer.from_pretrained(mistral_model)\n\ntokenizer.apply_chat_template(chat, tokenize=False)\n\n# he tokenizer has added the control tokens [INST] and [/INST] to indicate the \n# start and end of user messages (but not assistant messages!)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:40:39.905329Z","iopub.execute_input":"2024-02-06T13:40:39.905723Z","iopub.status.idle":"2024-02-06T13:40:42.465028Z","shell.execute_reply.started":"2024-02-06T13:40:39.905693Z","shell.execute_reply":"2024-02-06T13:40:42.463913Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddebce6ad14f469588f70135f4bea81e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41292fd5d65477a81a772029dad94c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7da0bb07b3e4d9cbe495d73b2028f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"834464ebfa3c4f5e92f152d17155e56b"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'<s>[INST] Hello, How are you [/INST]I am doing good. How can I help you</s> [INST] How can I show off chat template [/INST]'"},"metadata":{}}]},{"cell_type":"markdown","source":"When using chat templates as input for model generation, it’s also a good idea to use add_generation_prompt=True to add a generation prompt.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\ncheckpoint = \"HuggingFaceH4/zephyr-7b-beta\"  # 15GB model\n\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmessages = [\n    {\"role\": \"system\",\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ]\n\ntokenized_chat = tokenizer.apply_chat_template(messages,\n                                               tokenize=True,\n                                               add_generation_prompt=True, return_tensors=\"pt\")\n\nprint(tokenizer.decode(tokenized_chat[0]))\n\n# if using fb_model, the system followed by user roles threw error","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:44:54.570635Z","iopub.execute_input":"2024-02-06T13:44:54.571074Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d48a96f41045d9a62771b769ab8392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89d6603adeb49ae8efb462920427f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f27270c02c4a6aaf896606c662a312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c391b6fee5f40fea5a3212b1a782e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad53bc6177048dbb99cd27ef11978c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6984934ae4d24ae183fe4fe540fe22f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2201bf111fe43179438942178462b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9fd6d10b9a44c992b8a964aa2b7cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509c7f93386041b19f784c8eb767f9af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec474df356c4d4b953ce98199ae378f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc13936fc9444e7ad5d2f9065b55cfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa02b8670603436c88e2684602fa9a9d"}},"metadata":{}}]},{"cell_type":"code","source":"outputs = model.generate(tokenized_chat, max_new_tokens=128) \nprint(tokenizer.decode(outputs[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"conversational\", \"HuggingFaceH4/zephyr-7b-beta\")\n# The pipeline type called conversational, used for chat modeling.\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprint(pipe(messages))\n\n# ConversationalPipeline will take care of all the details of tokenization and calling \n# apply_chat_template for you - once the model has a chat template, ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"Hi there!\"},\n    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n\n\"\"\"<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"s. Some models, like **BlenderBot** and **LLaMA**, don’t have any special tokens before bot responses. ","metadata":{}},{"cell_type":"code","source":"tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\"\"\"<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n<|im_start|>assistant\n\"\"\"\n# note the <|im_start|>assistant at the end. Will prompt for assistant to respond","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\nchat1 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\n\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"],\n                                                                                 tokenize=False,\n                                                                                 add_generation_prompt=False)\n                                }\n                     )\nprint(dataset['formatted_chat'][0])","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:52:56.005041Z","iopub.execute_input":"2024-02-06T13:52:56.005711Z","iopub.status.idle":"2024-02-06T13:52:59.005116Z","shell.execute_reply.started":"2024-02-06T13:52:56.005676Z","shell.execute_reply":"2024-02-06T13:52:59.003698Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d63466885346828d43300afa89b266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc8bdcd1ce44a6c8dcf0529d7313059"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ce1362b4f3440181f9c3af3b2f73ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e6bad492a641ed8fd52cd64ac1055d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c68e0689554bb08241464e34db9adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e286c7276c4dd1873e0095112fc458"}},"metadata":{}},{"name":"stdout","text":"<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:54:13.774487Z","iopub.execute_input":"2024-02-06T13:54:13.775149Z","iopub.status.idle":"2024-02-06T13:54:14.332614Z","shell.execute_reply.started":"2024-02-06T13:54:13.775116Z","shell.execute_reply":"2024-02-06T13:54:14.331150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from rich import print\n\nprint(tokenizer.default_chat_template)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:54:30.793856Z","iopub.execute_input":"2024-02-06T13:54:30.794270Z","iopub.status.idle":"2024-02-06T13:54:30.809752Z","shell.execute_reply.started":"2024-02-06T13:54:30.794239Z","shell.execute_reply":"2024-02-06T13:54:30.808967Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\nNo chat template is defined for this tokenizer - using the default template for the BlenderbotTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1m{\u001b[0m% for message in messages %\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m% if message\u001b[1m[\u001b[0m\u001b[32m'role'\u001b[0m\u001b[1m]\u001b[0m == \u001b[32m'user'\u001b[0m %\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m \u001b[32m' '\u001b[0m \u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m% endif %\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m message\u001b[1m[\u001b[0m\u001b[32m'content'\u001b[0m\u001b[1m]\u001b[0m \u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m% if \nnot loop.last %\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m \u001b[32m'  '\u001b[0m \u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m% endif %\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m% endfor %\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m eos_token \u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>% for message in messages %<span style=\"font-weight: bold\">}{</span>% if message<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span><span style=\"font-weight: bold\">]</span> == <span style=\"color: #008000; text-decoration-color: #008000\">'user'</span> %<span style=\"font-weight: bold\">}{{</span> <span style=\"color: #008000; text-decoration-color: #008000\">' '</span> <span style=\"font-weight: bold\">}}{</span>% endif %<span style=\"font-weight: bold\">}{{</span> message<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span><span style=\"font-weight: bold\">]</span> <span style=\"font-weight: bold\">}}{</span>% if \nnot loop.last %<span style=\"font-weight: bold\">}{{</span> <span style=\"color: #008000; text-decoration-color: #008000\">'  '</span> <span style=\"font-weight: bold\">}}{</span>% endif %<span style=\"font-weight: bold\">}{</span>% endfor %<span style=\"font-weight: bold\">}{{</span> eos_token <span style=\"font-weight: bold\">}}</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"{% for message in messages %}\n    \n    {% if message['role'] == 'user' %}\n    \n        {{ ' ' }}\n    \n    {% endif %}\n    \n    {{ message['content'] }}\n    \n    {% if not loop.last %}\n    \n        {{ '  ' }}\n    \n    {% endif %}\n\n{% endfor %}\n\n{{ eos_token }}","metadata":{}},{"cell_type":"code","source":"for idx, message in enumerate(messages):\n    if message['role'] == 'user':\n        print(' ')\n    print(message['content'])\n    if not idx == len(messages) - 1:  # Check for the last message in the conversation\n        print('  ')\nprint(eos_token)\n\n\"\"\"\nEffectively, the template does three things:\n\nFor each message, if the message is a user message, add a blank space before it,\notherwise print nothing.\n\nAdd the message content\n\nIf the message is not the last message, add two spaces after it. \nAfter the final message, print the EOS token.\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"template = {% for message in messages %}\n    \n    {% if message['role'] == 'user' %}\n    \n        {{ bos_token + '[INST] ' + message['content'] + ' [/INST]' }}\n    \n    {% elif message['role'] == 'system' %}\n    \n        {{ '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}\n    \n    {% elif message['role'] == 'assistant' %}\n        \n        {{ ' '  + message['content'] + ' ' + eos_token }}\n    \n    {% endif %}\n\n{% endfor %}\n\n###  it adds specific tokens based on the “role” of each message, which represents who sent it. User, assistant and system messages are clearly distinguishable to the model because of the tokens they’re wrapped in. \n\nYou may find it easier to start with an existing template from another model and simply edit it for your needs! For example, we could take the LLaMA template above and add ”[ASST]” and ”[/ASST]” to assistant messages:","metadata":{}},{"cell_type":"code","source":"template = tokenizer.chat_template\ntemplate = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\ntokenizer.chat_template = template  # Set the new template\ntokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can find out what the **default template** for your tokenizer is by checking the tokenizer.default_chat_template attribute.\n\n### How to choose the template\n\n- ensure that the template exactly matches the message formatting that the model saw during training\n\n- you will probably get the best performance if you keep the chat tokens constant\n\n- the ChatML format, and this is a good, flexible choice for many use-cases","metadata":{}},{"cell_type":"markdown","source":"{% for message in messages %}\n    \n    {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n\n{% endfor %}","metadata":{}},{"cell_type":"code","source":"# The one-liner also includes handy support for generation prompts, but note that\n# it doesn’t add BOS or EOS tokens! If your model expects those, \n# they won’t be added automatically by apply_chat_template - in other words, \n# the text will be tokenized with add_special_tokens=False.\n\ntokenizer.chat_template = \"\"\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-06T14:03:26.229106Z","iopub.execute_input":"2024-02-06T14:03:26.229604Z","iopub.status.idle":"2024-02-06T14:03:26.236298Z","shell.execute_reply.started":"2024-02-06T14:03:26.229534Z","shell.execute_reply":"2024-02-06T14:03:26.235089Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"If you’re unfamiliar with Jinja, we generally find that the easiest way to write a chat template is to first write a short Python script that formats messages the way you want, and then convert that script into a template.\n\n> for loop:\n\n{% for message in messages %}\n\n{{ message['content'] }}\n\n{% endfor %}\n\n> {{ expression block }} is printed\n\n> If Block:\n\n{% if message['role'] == 'user' %}\n\n{{ message['content'] }}\n\n{% endif %}\n\n> Jinja requires you to explicitly end them with {% endfor %} and {% endif %}.\n\n> Inside your template, you will have access to the list of messages, but you can also access several other special variables. These include special tokens like bos_token and eos_token, as well as the add_generation_prompt variable that we discussed above\n\n> , we’ve tried to get Jinja to ignore whitespace outside of {{ expressions }}. However, be aware that Jinja is a general-purpose templating engine, and it may treat whitespace between blocks on the same line as significant and print it to the output","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}