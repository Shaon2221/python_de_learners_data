{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers >> /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models classes \n\n- have the methods for loading model from hub / or from directory\n\n- resize the input token embeddings when new tokens are added\n\n- prune the attention heads of the model\n\n- Mixins with Additional Methods: \n    \n    > ModuleUtilMixin (pytorch models)\n    \n    > GenerationMixin (Generation models)","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the model \nfrom transformers import AutoModel, AutoTokenizer\nimport numpy as np\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n\nmodel.add_model_tags([\"custom\", \"custom_bert\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"custom_bert\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.can_generate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModel.from_pretrained(\"bert-base-cased\", output_attentions=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert model.config.output_attentions == True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**low_cpu_mem_usage algorithm:**\n\nThis is an experimental function that loads the model using ~1x model size CPU memory\n\nHere is how it works:\n\n- save which state_dict keys we have\n\n- drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n\n- after the model has been instantiated switch to the meta device all params/buffers that are going to be replaced from the loaded state_dict\n\n- load state_dict 2nd time\n\n- replace the params/buffers from the state_dict\n\n- there is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded\n\n\n\n<> torch_dtype (str or torch.dtype, optional) — Override the default torch.dtype and load the model under a specific dtype. \n\n> torch.float16 or torch.bfloat16 or torch.float: load in a specified dtype, ignoring the model’s config.torch_dtype if one exists. If not specified\n\n\n> \"auto\" - A torch_dtype entry in the config.json file of the model will be attempted to be used. \n\n<> device_map  — A map that specifies where each submodule should go. If we only pass the device (e.g., \"cpu\", \"cuda:1\", \"mps\", or a GPU ordinal rank like 1) on which the model will be allocated, the device map will map the entire model to this device. \n\n> Passing device_map = 0 means put the whole model on GPU 0.","metadata":{}},{"cell_type":"code","source":"# provides the size of the model\nmodel.get_memory_footprint(return_buffers=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_output_embeddings()  # no output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_input_embeddings()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, parm in model.named_parameters():\n    print(name, parm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install optimum >> /dev/null","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pymodel = model.to_bettertransformer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\nimport torch\n\n# will throw torch not compiled with CUDA if there is no GPU enabled\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-base\",\n                                                   torch_dtype=torch.float16,\n                                                   device_map=\"auto\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokeniser = T5Tokenizer.from_pretrained(\"google-t5/t5-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.hf_device_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sent = \"This sentence is used for testing\"\ninput = tokeniser(test_sent)\nids = input['input_ids']\nids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"getting the error AttributeError: 'list' object has no attribute 'numel'","metadata":{}},{"cell_type":"code","source":"model.estimate_tokens(input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.floating_point_ops(input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.num_parameters(only_trainable=False, exclude_embeddings=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generation Configuration\n\nClass that holds a configuration for a generation task. A generate call supports the following generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n\n- greedy decoding by calling greedy_search() if num_beams=1 and do_sample=False\n\n- contrastive search by calling contrastive_search() if penalty_alpha>0. and top_k>1\n\n- multinomial sampling by calling sample() if num_beams=1 and do_sample=True\n\n- beam-search decoding by calling beam_search() if num_beams>1 and do_sample=False\n\n- beam-search multinomial sampling by calling beam_sample() if num_beams>1 and do_sample=True\n\n- diverse beam-search decoding by calling group_beam_search(), if num_beams>1 and num_beam_groups>1\n\n- constrained beam-search decoding by calling constrained_beam_search(), if constraints!=None or force_words_ids!=None\n\n- assisted decoding by calling assisted_decoding(), if assistant_model is passed to .generate()\n\nYou do not need to call any of the above methods directly. Pass custom parameter values to ‘.generate()‘. To learn more about decoding strategies refer to the text generation strategies guide","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig\n\ngeneration_config = GenerationConfig.from_pretrained(\"gpt2\", top_k=1, do_sample=True,\n                                                    return_unused_kwargs=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generation_config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer\n\ntokeniser = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokeniser.pad_token_id = tokeniser.eos_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prompt... ing\n\ntest = 'Today is'\n\ninputs = tokeniser([test], return_tensors='pt')\ninputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example 1: print the scores of output, generated with greedy search\n\noutputs = model.generate(**inputs,\n                         max_new_tokens=5,\n                         return_dict_in_generate=True,\n                        output_scores=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Computes the **transition scores** of sequences given the generation scores (and beam indices, if beam search was used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time","metadata":{}},{"cell_type":"code","source":"trans_scores = model.compute_transition_scores(outputs.sequences,\n                                              outputs.scores, normalize_logits=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_length is the length of the input prompt for decoder-only models,\n# like the GPT family, and 1 for encoder-decoder family like Bart / T5\n\ninput_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_tokens = outputs.sequences[:, input_length:]\ngen_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tok, score in zip(gen_tokens[0], trans_scores[0]):\n    print(f\"| {tok:5d} | {tokeniser.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example 2 with beam search\n\noutputs = model.generate(**inputs, max_new_tokens=5, num_beams=4,\n                        num_return_sequences=4, return_dict_in_generate=True,\n                        output_scores=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_scores = model.compute_transition_scores(outputs.sequences, outputs.scores,\n                                              outputs.beam_indices, normalize_logits=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_length = np.sum(trans_scores.numpy() < 0, axis=1)\n\nlength_penalty = model.generation_config.length_penalty\n\nreconstructed_scores = trans_scores.sum(axis=1) / (output_length**length_penalty)\n\nprint(np.allclose(outputs.sequences_scores, reconstructed_scores))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# greedy decoding and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    LogitsProcessorList,\n    MinLengthLogitsProcessor,\n    StoppingCriteriaList,\n    MaxLengthCriteria\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModel.from_pretrained(\"gpt2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.generation_config)\n# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n\nprint(model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generation_config = GenerationConfig.from_pretrained(\"gpt2\", top_k=1, do_sample=True,\n                                                    return_unused_kwargs=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calling to_dict fills up whole lot of parameters\ngeneration_config[0].to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# following was done to get greedy search work\n# model.generation_config = {\"pad_token_id\": model.config.eos_token_id} # not working\n# model.generation_config.pad_token_id = model.config.eos_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_prompt = \"It might be possible to\"\ninput_ids = tokenizer(input_prompt, return_tensors='pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits_processor = LogitsProcessorList(\n    [\n        MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id)\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# greedy search did not execute, as the model was not having the \n# generation config\noutputs = model.greedy_search(\n    input_ids['input_ids'],\n    logits_processor=logits_processor,\n    stopping_criteria=stopping_criteria,\n    pad_token_id = model.config.eos_token_id,\n    eos_token_id = model.config.eos_token_id,\n    output_scores = True,\n    max_new_tokens=5,\n    num_return_sequences=4,\n    return_dict_in_generate=True,\n    output_attentions = False,\n    output_hidden_states = False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # multinomial sampling and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    LogitsProcessorList,\n    MinLengthLogitsProcessor,\n    TopKLogitsWarper,\n    TemperatureLogitsWarper,\n    StoppingCriteriaList,\n    MaxLengthCriteria,\n)\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set pad_token_id to eos_token_id because GPT2 does not have a EOS token\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.generation_config.pad_token_id = model.config.eos_token_id\n\ninput_prompt = \"Today is a beautiful day, and\"\ninput_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.generation_config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate logits processors\nlogits_processor = LogitsProcessorList(\n    [\n        MinLengthLogitsProcessor(15, \n                                 eos_token_id=model.generation_config.eos_token_id),\n    ]\n)\n# instantiate logits processors\nlogits_warper = LogitsProcessorList(\n    [\n        TopKLogitsWarper(50),\n        TemperatureLogitsWarper(0.7),\n    ]\n)\n\nstopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\n# uses multinomial sampling to generate the sequences\n\noutputs = model.sample(\n    input_ids,\n    logits_processor=logits_processor,\n    logits_warper=logits_warper,  # this is added in sample method\n    stopping_criteria=stopping_criteria,\n)\n\ntokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beam search is used for  used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    BeamSearchScorer, # this is new class in Beam Search\n    AutoModelForSeq2SeqLM\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\nencoder_input_str = \"translate English to German: How young are you?\"\nencoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n\n# lets run beam search using 3 beams\nnum_beams = 3\n\n# define decoder start token ids\ninput_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\ninput_ids = input_ids * model.config.decoder_start_token_id\n\n# add encoder_outputs to model keyword arguments\nmodel_kwargs = {\n    \"encoder_outputs\": model.get_encoder()(\n        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n    )\n}\n\n# instantiate beam scorer\nbeam_scorer = BeamSearchScorer(\n    batch_size=1,\n    num_beams=num_beams,\n    device=model.device,\n)\n\n# instantiate logits processors\nlogits_processor = LogitsProcessorList(\n    [\n        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n    ]\n)\n\noutputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beam search multinomial sample is used for  used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = model.get_encoder()\nencoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = model.get_decoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    LogitsProcessorList,\n    MinLengthLogitsProcessor,\n    TopKLogitsWarper,\n    TemperatureLogitsWarper,\n    BeamSearchScorer,\n)\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\nencoder_input_str = \"translate English to German: How young are you?\"\nencoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n# lets run beam search using 3 beams\nnum_beams = 3\n\n# define decoder start token ids\ninput_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\ninput_ids = input_ids * model.config.decoder_start_token_id\n\n# add encoder_outputs to model keyword arguments\nmodel_kwargs = {\n    \"encoder_outputs\": model.get_encoder()(\n        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n    )\n}\n\n# instantiate beam scorer\nbeam_scorer = BeamSearchScorer(\n    batch_size=1,\n    max_length=model.config.max_length,\n    num_beams=num_beams,\n    device=model.device,\n)\n\n# instantiate logits processors\nlogits_processor = LogitsProcessorList(\n    [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]\n)\n# instantiate logits processors\nlogits_warper = LogitsProcessorList(\n    [\n        TopKLogitsWarper(50),\n        TemperatureLogitsWarper(0.7),\n    ]\n)\n\noutputs = model.beam_sample(\n    input_ids,\n    beam_scorer,\n    logits_processor=logits_processor,\n    logits_warper=logits_warper,\n    **model_kwargs\n)\n\ntokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# contrastive search used for text-decoder, text-to-text, speech-to-text, and vision-to-text models\n# constrained beam search decoding and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n# diverse beam search decoding and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Outputs","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:25.294272Z","iopub.execute_input":"2024-02-01T01:09:25.294737Z","iopub.status.idle":"2024-02-01T01:09:29.117521Z","shell.execute_reply.started":"2024-02-01T01:09:25.294697Z","shell.execute_reply":"2024-02-01T01:09:29.116495Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.tensor([1]).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:33.147112Z","iopub.execute_input":"2024-02-01T01:09:33.147667Z","iopub.status.idle":"2024-02-01T01:09:33.156318Z","shell.execute_reply.started":"2024-02-01T01:09:33.147639Z","shell.execute_reply":"2024-02-01T01:09:33.155153Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"tensor([[1]])"},"metadata":{}}]},{"cell_type":"code","source":"inputs = tokenizer(\"Hello, this is a superb day.\", return_tensors='pt')\nlabels = torch.tensor([1]).unsqueeze(0)\noutputs = model(**inputs, labels=labels)  # why labels are passed?","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:35.084928Z","iopub.execute_input":"2024-02-01T01:09:35.085316Z","iopub.status.idle":"2024-02-01T01:09:35.175818Z","shell.execute_reply.started":"2024-02-01T01:09:35.085290Z","shell.execute_reply":"2024-02-01T01:09:35.174885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"outputs","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:37.476761Z","iopub.execute_input":"2024-02-01T01:09:37.477221Z","iopub.status.idle":"2024-02-01T01:09:37.484954Z","shell.execute_reply.started":"2024-02-01T01:09:37.477192Z","shell.execute_reply":"2024-02-01T01:09:37.484121Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"SequenceClassifierOutput(loss=tensor(0.3689, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2489,  0.5581]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"},"metadata":{}}]},{"cell_type":"code","source":"outputs_wolabel = model(**inputs)\noutputs_wolabel # will not have loss, as the labels are not passed","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:43.254181Z","iopub.execute_input":"2024-02-01T01:09:43.254597Z","iopub.status.idle":"2024-02-01T01:09:43.336107Z","shell.execute_reply.started":"2024-02-01T01:09:43.254571Z","shell.execute_reply":"2024-02-01T01:09:43.335268Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"SequenceClassifierOutput(loss=None, logits=tensor([[-0.2489,  0.5581]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"},"metadata":{}}]},{"cell_type":"code","source":"outputs = model(**inputs,\n                labels=labels,\n                output_hidden_states=True,\n                output_attentions=True)\noutputs","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:52.360816Z","iopub.execute_input":"2024-02-01T01:09:52.361245Z","iopub.status.idle":"2024-02-01T01:09:52.485389Z","shell.execute_reply.started":"2024-02-01T01:09:52.361217Z","shell.execute_reply":"2024-02-01T01:09:52.484178Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"SequenceClassifierOutput(loss=tensor(0.3689, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2489,  0.5581]], grad_fn=<AddmmBackward0>), hidden_states=(tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n           3.8253e-02,  1.6400e-01],\n         [ 3.7386e-01, -1.5575e-02, -2.4561e-01,  ..., -3.1657e-02,\n           5.5144e-01, -5.2406e-01],\n         [ 4.6704e-04,  1.6225e-01, -6.4443e-02,  ...,  4.9443e-01,\n           6.9413e-01,  3.6286e-01],\n         ...,\n         [-2.6303e-01,  1.4989e-01,  1.8093e-01,  ...,  2.4644e-01,\n           8.5299e-03, -6.3424e-01],\n         [-1.5500e-01,  6.9230e-02, -1.6601e-01,  ...,  4.3867e-01,\n           6.4413e-01,  5.9384e-01],\n         [-1.4736e-01, -4.1137e-02, -7.3157e-02,  ..., -1.1568e-01,\n           4.2107e-02, -5.4994e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0605,  0.0289, -0.1973,  ...,  0.2396, -0.1291, -0.0037],\n         [ 0.3784,  0.0968,  0.2672,  ...,  0.0520,  0.5898, -0.3454],\n         [-0.3904,  0.3737, -0.1842,  ..., -0.1026,  0.6340,  0.1310],\n         ...,\n         [-0.5331, -0.2153,  0.3669,  ..., -0.2129, -0.0531, -1.1592],\n         [-0.1598, -0.1429, -0.0976,  ...,  0.2373,  0.2719,  0.3365],\n         [-0.0842, -0.0140, -0.0212,  ...,  0.0831,  0.2211,  0.1186]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0348, -0.2010, -0.4044,  ...,  0.4550,  0.0557,  0.0224],\n         [ 0.5786,  0.4823,  1.1406,  ...,  1.0759,  1.0092, -0.7571],\n         [-0.4266,  0.4545, -0.2455,  ...,  0.0180,  0.1270, -0.0515],\n         ...,\n         [-0.5820, -0.4809,  0.4654,  ...,  0.0035, -0.2522, -1.3615],\n         [-0.1250, -0.1392,  0.1743,  ...,  0.1762,  0.2946,  0.5347],\n         [-0.1637, -0.0758,  0.0282,  ...,  0.1938,  0.2716, -0.1207]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0125, -0.3533, -0.0829,  ...,  0.4876,  0.2163,  0.1932],\n         [ 0.8078,  0.1295,  1.3233,  ...,  0.9614,  0.4673, -0.5715],\n         [-0.1867,  0.7249,  0.1170,  ..., -0.3343,  0.0086,  0.0070],\n         ...,\n         [-0.6010, -0.0370,  0.5881,  ..., -0.0585, -0.1869, -1.4461],\n         [-0.2526, -0.2252,  0.2910,  ...,  0.1604,  0.1910,  0.1131],\n         [-0.0544, -0.0948,  0.1158,  ...,  0.0772,  0.0661, -0.0402]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 9.6968e-02, -5.6033e-01, -5.5277e-01,  ...,  4.7938e-01,\n           3.7125e-01,  5.5757e-01],\n         [ 7.8442e-01, -1.0146e-02,  1.4294e+00,  ...,  1.1397e+00,\n           6.0763e-01, -3.7664e-01],\n         [-3.4199e-01,  8.6371e-01,  9.4455e-02,  ..., -5.1587e-02,\n           2.1072e-01,  2.8761e-01],\n         ...,\n         [-4.1377e-01, -2.7254e-01,  3.2697e-02,  ...,  3.6906e-01,\n          -6.5632e-01, -1.3115e+00],\n         [-1.5096e-01, -1.3468e-01,  7.7760e-02,  ..., -1.2696e-01,\n           1.0482e-01,  3.5561e-01],\n         [-2.0617e-02, -5.3334e-02, -1.0912e-03,  ...,  5.0751e-03,\n           5.4303e-02, -3.4522e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1250, -0.4860, -0.3794,  ...,  0.0221,  0.3550,  0.5438],\n         [ 0.8964,  0.0768,  0.8924,  ...,  0.9631,  0.8251, -0.2312],\n         [ 0.2314,  0.7270, -0.2072,  ..., -0.3125,  0.1668,  0.7413],\n         ...,\n         [-0.7453, -0.0618,  0.2954,  ...,  0.3848, -0.8042, -1.3137],\n         [ 0.1729, -0.2096,  0.0204,  ...,  0.0323,  0.0191,  0.4243],\n         [-0.0190, -0.0320,  0.0146,  ...,  0.0207,  0.0105, -0.0380]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3269, -0.8443, -0.2549,  ..., -0.0836,  0.5745,  0.3666],\n         [ 1.0258, -0.2951,  0.8763,  ...,  0.7262,  0.4890, -0.1104],\n         [ 0.2473,  0.8955, -0.0827,  ..., -0.1675,  0.3451,  0.9892],\n         ...,\n         [-0.8024,  0.0456,  0.5226,  ...,  0.0976, -0.8850, -1.6883],\n         [ 0.2770,  0.1147, -0.0933,  ..., -0.1700, -0.1590,  0.3458],\n         [ 0.0195, -0.0429, -0.0164,  ...,  0.0035, -0.0109, -0.0427]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3132, -0.6840,  0.0036,  ..., -0.5601,  0.5962,  0.3249],\n         [ 0.7698, -0.2175,  0.5198,  ...,  0.7758,  0.7833, -0.1439],\n         [ 0.0451,  0.8726,  0.3218,  ..., -0.0157,  1.0504,  1.0816],\n         ...,\n         [-0.5780,  0.1248,  0.4861,  ...,  0.4597, -1.1065, -1.8598],\n         [-0.1221, -0.2163, -0.3687,  ..., -0.5963, -0.0091,  0.6188],\n         [ 0.0062, -0.0294, -0.0070,  ..., -0.0223,  0.0241, -0.0543]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2529, -0.3460, -0.2738,  ..., -0.7554,  0.5656,  0.7889],\n         [ 0.8401,  0.1529,  0.7832,  ...,  0.1729,  1.0543,  0.1620],\n         [-0.1240,  0.8355,  0.6320,  ..., -0.3053,  0.7507,  0.9400],\n         ...,\n         [-0.3168, -0.0369,  0.3298,  ...,  0.5919, -0.4944, -1.4215],\n         [-0.5773, -0.2534, -0.5345,  ..., -0.6358, -0.2431,  0.4469],\n         [ 0.0232, -0.0212,  0.0455,  ..., -0.0284, -0.0176, -0.0671]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2655, -0.1650, -0.3564,  ..., -0.0346,  0.2049,  0.8729],\n         [ 0.8506,  0.2967,  0.7617,  ..., -0.2790,  0.8801,  0.2560],\n         [-0.0655,  0.6910,  0.8040,  ..., -0.1459,  0.3396,  0.7855],\n         ...,\n         [ 0.0038, -0.0088,  0.1998,  ...,  0.4972, -0.6497, -1.3749],\n         [-0.4857,  0.0852, -0.5857,  ..., -0.7706, -0.4502, -0.0794],\n         [ 0.0139, -0.0297,  0.0388,  ..., -0.0209, -0.0540, -0.0123]]],\n       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.4946e-01,  7.7625e-02, -4.7400e-03,  ..., -2.9341e-01,\n          -3.5194e-01,  7.4265e-01],\n         [ 1.0093e+00,  2.6339e-01,  1.0276e+00,  ..., -7.5460e-01,\n           6.9929e-01,  1.1865e-01],\n         [-2.6095e-01,  4.1184e-01,  1.1626e+00,  ..., -9.1198e-01,\n           4.0703e-01,  4.0723e-01],\n         ...,\n         [ 1.2896e-01, -2.3278e-01, -1.6012e-01,  ...,  2.1762e-01,\n          -5.2960e-01, -1.2932e+00],\n         [-2.5203e-02, -1.3750e-02, -6.0961e-02,  ..., -7.7345e-02,\n          -2.2329e-02,  1.0313e-03],\n         [-9.5408e-03, -1.1094e-02,  1.9958e-01,  ...,  8.1562e-02,\n           1.5788e-02, -6.2980e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.5087e-01,  9.5998e-02,  3.8922e-01,  ..., -3.6492e-01,\n           2.0952e-01,  3.1219e-01],\n         [ 1.2692e+00,  2.6083e-01,  7.2847e-01,  ..., -9.3267e-01,\n           1.0367e+00,  1.5284e-01],\n         [-1.6650e-01,  1.9654e-01,  7.0021e-01,  ..., -7.3923e-01,\n           8.1174e-01,  4.2555e-01],\n         ...,\n         [ 2.2177e-01, -4.1640e-01, -2.1932e-01,  ...,  1.9110e-01,\n          -3.6365e-02, -1.3104e+00],\n         [ 4.0203e-02,  1.5501e-02, -2.7669e-02,  ...,  1.2414e-02,\n          -1.7388e-02, -4.0865e-03],\n         [ 3.0501e-02,  4.3070e-04, -1.2934e-02,  ...,  1.9559e-02,\n          -1.5733e-02, -3.7588e-04]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1155,  0.1455, -0.0994,  ..., -0.4279,  0.3301,  0.7040],\n         [ 0.8888,  0.3977,  0.5624,  ..., -0.2613,  1.1559,  0.3556],\n         [-0.0982,  0.5027,  0.6323,  ..., -0.3455,  0.5989,  0.3928],\n         ...,\n         [ 0.0611, -0.5388, -0.1237,  ...,  0.2615,  0.0111, -0.6936],\n         [ 0.6163,  0.0838, -0.4173,  ...,  0.1700, -0.3724, -0.3174],\n         [ 0.6290,  0.1287, -0.2074,  ...,  0.1447, -0.4940, -0.2156]]],\n       grad_fn=<NativeLayerNormBackward0>)), attentions=(tensor([[[[6.1554e-02, 4.7203e-02, 5.0031e-02,  ..., 4.4227e-02,\n           1.2172e-01, 3.1309e-01],\n          [8.4047e-02, 1.0333e-01, 4.4906e-02,  ..., 2.0950e-01,\n           4.4737e-02, 1.2410e-01],\n          [8.2368e-02, 9.1782e-02, 7.0766e-02,  ..., 1.1751e-01,\n           1.1972e-01, 1.1282e-01],\n          ...,\n          [6.7200e-02, 8.5108e-02, 1.3865e-01,  ..., 2.4098e-02,\n           9.6323e-02, 9.9931e-02],\n          [7.6078e-02, 7.4662e-02, 1.2442e-01,  ..., 1.0520e-01,\n           1.4671e-01, 1.0276e-01],\n          [8.1784e-02, 6.2856e-02, 1.1549e-01,  ..., 5.3279e-02,\n           1.4541e-01, 1.6067e-01]],\n\n         [[4.2165e-01, 1.1382e-03, 5.2747e-01,  ..., 3.3001e-03,\n           2.2014e-02, 4.5052e-03],\n          [6.6770e-02, 2.4203e-01, 3.5874e-02,  ..., 7.8375e-02,\n           3.9673e-02, 1.0167e-01],\n          [1.4161e-03, 7.5613e-02, 2.7077e-02,  ..., 2.6190e-02,\n           1.0736e-01, 7.3183e-03],\n          ...,\n          [5.7163e-03, 3.1714e-01, 8.3856e-03,  ..., 8.9589e-02,\n           1.9604e-02, 9.5331e-02],\n          [9.4036e-04, 2.6238e-02, 2.7606e-02,  ..., 2.7068e-02,\n           2.2327e-01, 1.1154e-02],\n          [2.5541e-02, 2.1001e-02, 3.2553e-01,  ..., 1.9199e-02,\n           3.0139e-01, 2.4880e-02]],\n\n         [[7.7993e-01, 1.7280e-02, 2.6526e-02,  ..., 1.4417e-02,\n           3.0550e-02, 5.0952e-02],\n          [9.3119e-01, 1.8491e-03, 1.5332e-02,  ..., 4.5714e-04,\n           3.9116e-03, 1.2993e-02],\n          [3.5772e-01, 3.9092e-01, 1.1604e-01,  ..., 2.4984e-03,\n           2.2658e-03, 1.6092e-02],\n          ...,\n          [2.3018e-01, 2.5092e-02, 1.2444e-02,  ..., 3.5503e-02,\n           4.9893e-02, 5.5889e-02],\n          [1.2535e-01, 1.8047e-02, 3.3865e-02,  ..., 4.1132e-01,\n           1.5610e-01, 8.4145e-02],\n          [2.5860e-01, 5.2139e-04, 4.2494e-03,  ..., 2.4728e-02,\n           5.6012e-01, 1.0973e-01]],\n\n         ...,\n\n         [[6.9558e-02, 3.8778e-02, 4.0281e-01,  ..., 1.4803e-01,\n           3.6215e-02, 1.2747e-02],\n          [2.1980e-01, 2.0098e-01, 2.2692e-02,  ..., 1.0284e-01,\n           1.3543e-01, 1.9269e-01],\n          [4.3953e-01, 4.6809e-02, 8.0952e-02,  ..., 4.3559e-02,\n           5.6976e-02, 1.5542e-01],\n          ...,\n          [1.8383e-01, 4.5043e-02, 4.9792e-02,  ..., 3.7595e-01,\n           6.5225e-02, 1.3465e-01],\n          [5.6836e-01, 3.5471e-02, 1.1712e-01,  ..., 1.8473e-02,\n           1.6323e-02, 1.1467e-01],\n          [3.8425e-01, 6.5857e-02, 2.8175e-01,  ..., 4.1355e-02,\n           3.0987e-02, 2.0427e-02]],\n\n         [[8.3809e-01, 1.3970e-02, 1.7968e-02,  ..., 9.7834e-03,\n           1.9491e-02, 2.6347e-02],\n          [1.4869e-01, 5.6845e-02, 5.6100e-01,  ..., 9.4950e-03,\n           6.9302e-03, 2.6088e-03],\n          [2.1902e-03, 3.1566e-02, 4.7982e-02,  ..., 2.2985e-03,\n           3.8146e-03, 3.9991e-03],\n          ...,\n          [3.7858e-02, 6.2129e-04, 9.9679e-04,  ..., 3.4395e-02,\n           5.9029e-01, 2.0538e-01],\n          [5.6484e-03, 1.2627e-03, 1.8765e-04,  ..., 1.6870e-02,\n           4.0636e-02, 9.1061e-01],\n          [2.3152e-01, 3.6311e-03, 4.2757e-03,  ..., 4.5956e-02,\n           3.0834e-01, 3.7496e-01]],\n\n         [[8.7033e-01, 4.8617e-03, 1.7452e-06,  ..., 2.4989e-03,\n           4.5156e-07, 2.9775e-02],\n          [4.9866e-01, 1.0178e-02, 5.3021e-02,  ..., 1.9758e-02,\n           2.0929e-02, 8.2350e-02],\n          [4.9901e-01, 2.2643e-01, 1.0926e-03,  ..., 8.4898e-03,\n           4.0572e-04, 5.8974e-02],\n          ...,\n          [9.7942e-02, 3.5297e-02, 2.0486e-02,  ..., 8.4955e-02,\n           6.0838e-02, 1.5785e-01],\n          [5.0844e-01, 6.2297e-02, 5.8348e-04,  ..., 3.5922e-02,\n           1.6513e-03, 2.5218e-01],\n          [6.8095e-01, 8.4333e-03, 3.3462e-04,  ..., 2.2959e-02,\n           1.4971e-03, 1.5849e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.5124e-01, 2.4311e-02, 2.9384e-01,  ..., 9.0269e-02,\n           2.5443e-01, 3.1499e-02],\n          [2.6323e-01, 2.3380e-02, 1.2234e-01,  ..., 2.6513e-02,\n           1.3691e-01, 3.1603e-01],\n          [6.1759e-01, 1.6671e-02, 5.7004e-02,  ..., 3.2445e-02,\n           1.0408e-01, 9.2898e-02],\n          ...,\n          [2.0930e-01, 3.7696e-02, 9.9428e-02,  ..., 1.1999e-02,\n           1.4519e-01, 2.6244e-01],\n          [4.4593e-01, 2.5016e-02, 1.0109e-01,  ..., 6.0738e-02,\n           1.9709e-01, 6.7650e-02],\n          [3.4858e-01, 2.6369e-02, 1.6470e-01,  ..., 9.0560e-02,\n           1.8774e-01, 4.2200e-02]],\n\n         [[4.7175e-01, 5.8382e-02, 9.0233e-02,  ..., 2.4862e-02,\n           7.1227e-02, 1.1818e-01],\n          [7.3217e-01, 7.9081e-03, 2.4121e-01,  ..., 2.7116e-03,\n           1.0953e-03, 8.9520e-04],\n          [2.5101e-01, 8.8509e-03, 2.7720e-02,  ..., 7.6074e-04,\n           1.6501e-02, 3.9136e-03],\n          ...,\n          [1.9503e-01, 1.3827e-04, 1.0993e-03,  ..., 4.8911e-02,\n           5.6148e-01, 1.4027e-01],\n          [5.0182e-01, 2.8347e-04, 2.0002e-05,  ..., 5.4761e-04,\n           2.3328e-02, 4.6886e-01],\n          [8.9119e-01, 1.0452e-03, 9.5961e-04,  ..., 9.7425e-04,\n           8.1689e-03, 9.3760e-02]],\n\n         [[8.2212e-01, 1.8931e-02, 1.6579e-02,  ..., 2.1771e-02,\n           2.7723e-02, 4.9158e-02],\n          [2.7553e-01, 1.3086e-02, 3.2650e-02,  ..., 3.1989e-02,\n           9.8292e-02, 2.1534e-01],\n          [3.0541e-01, 1.0613e-01, 2.1244e-02,  ..., 3.5829e-02,\n           6.3392e-02, 2.3307e-01],\n          ...,\n          [1.2799e-01, 1.8823e-01, 2.5618e-02,  ..., 1.0231e-01,\n           8.2262e-02, 1.3794e-01],\n          [2.9122e-01, 1.1854e-01, 1.9782e-02,  ..., 2.0841e-02,\n           4.9167e-02, 2.4194e-01],\n          [6.4115e-01, 6.0358e-02, 1.4354e-02,  ..., 1.5538e-02,\n           3.0726e-02, 1.4456e-01]],\n\n         ...,\n\n         [[1.8303e-01, 3.0180e-02, 1.0327e-01,  ..., 4.1890e-02,\n           1.2582e-01, 1.5156e-01],\n          [4.6748e-01, 1.5510e-03, 4.1880e-02,  ..., 3.1749e-02,\n           7.6916e-02, 2.5338e-01],\n          [3.7606e-01, 7.9607e-02, 3.4706e-02,  ..., 3.0394e-02,\n           1.0027e-01, 1.8237e-01],\n          ...,\n          [2.7885e-01, 8.4375e-02, 1.0526e-01,  ..., 1.7138e-02,\n           8.9637e-02, 2.1697e-01],\n          [2.6557e-01, 7.4693e-02, 9.3102e-02,  ..., 5.4532e-02,\n           9.5864e-02, 1.6075e-01],\n          [8.8224e-02, 5.2651e-02, 1.5660e-01,  ..., 5.9696e-02,\n           1.0756e-01, 1.5258e-01]],\n\n         [[5.8863e-01, 2.1045e-02, 5.2010e-02,  ..., 1.6603e-02,\n           7.8119e-02, 1.4811e-01],\n          [7.2499e-01, 5.3030e-03, 2.7188e-02,  ..., 3.6309e-02,\n           1.5134e-02, 2.5129e-02],\n          [5.6572e-01, 7.4456e-02, 7.2127e-02,  ..., 2.2592e-02,\n           3.0582e-02, 4.7320e-02],\n          ...,\n          [5.0518e-02, 2.4612e-02, 1.3870e-02,  ..., 4.6327e-02,\n           2.7609e-02, 6.3884e-02],\n          [1.6701e-01, 5.5824e-02, 8.4540e-02,  ..., 1.9784e-02,\n           8.3232e-02, 1.1431e-01],\n          [8.8204e-01, 6.1096e-03, 8.3890e-03,  ..., 7.5699e-03,\n           2.1904e-02, 4.7583e-02]],\n\n         [[2.6618e-01, 8.4198e-02, 4.7233e-02,  ..., 2.0485e-02,\n           4.8575e-02, 2.0812e-01],\n          [1.6424e-01, 5.2560e-01, 1.4058e-02,  ..., 1.5882e-02,\n           8.2892e-02, 1.5128e-01],\n          [2.4224e-01, 2.4328e-02, 2.5365e-02,  ..., 2.7739e-02,\n           9.0129e-02, 4.2655e-01],\n          ...,\n          [8.8197e-02, 3.4975e-02, 3.6660e-02,  ..., 6.3032e-01,\n           1.4842e-02, 1.3303e-01],\n          [1.8194e-01, 1.2708e-01, 9.6988e-02,  ..., 1.0314e-02,\n           1.8394e-02, 9.1814e-02],\n          [2.5102e-01, 1.6553e-01, 1.0458e-01,  ..., 2.3239e-02,\n           3.6252e-02, 7.7646e-02]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[9.6566e-01, 4.1348e-04, 2.3470e-03,  ..., 7.6050e-04,\n           5.2869e-03, 2.4398e-02],\n          [1.9540e-06, 1.7134e-08, 9.9999e-01,  ..., 3.6488e-06,\n           7.3274e-08, 1.1462e-10],\n          [1.7566e-06, 3.2094e-06, 2.0661e-05,  ..., 2.6220e-09,\n           8.2934e-08, 1.4811e-07],\n          ...,\n          [1.7124e-06, 4.2497e-14, 3.8540e-07,  ..., 2.3721e-06,\n           9.9999e-01, 7.9779e-07],\n          [5.9307e-03, 2.1558e-07, 1.1579e-09,  ..., 2.5282e-06,\n           9.5049e-04, 9.9308e-01],\n          [9.8798e-01, 4.7405e-06, 1.1794e-04,  ..., 6.4647e-04,\n           3.8259e-04, 1.0819e-02]],\n\n         [[6.7395e-01, 1.9076e-02, 2.6023e-02,  ..., 5.5643e-03,\n           4.5107e-02, 1.6012e-01],\n          [8.4966e-01, 9.2042e-04, 4.0217e-02,  ..., 2.3530e-04,\n           6.0939e-03, 9.3008e-02],\n          [4.0882e-01, 4.4307e-01, 2.1159e-02,  ..., 2.8569e-03,\n           7.3519e-03, 3.1286e-02],\n          ...,\n          [4.2135e-02, 7.8356e-04, 6.1481e-02,  ..., 4.8730e-03,\n           1.2165e-01, 7.7419e-02],\n          [6.5994e-01, 1.2301e-02, 9.7912e-03,  ..., 2.2098e-02,\n           5.5174e-02, 1.4759e-01],\n          [8.1447e-01, 2.5411e-03, 3.3496e-03,  ..., 4.6154e-03,\n           2.3140e-02, 1.4230e-01]],\n\n         [[7.0392e-01, 4.0743e-02, 1.6095e-02,  ..., 9.1515e-03,\n           5.0309e-02, 1.2242e-01],\n          [3.4013e-01, 1.1548e-02, 1.6629e-01,  ..., 3.0478e-02,\n           1.9029e-01, 2.0938e-01],\n          [8.0221e-01, 2.7730e-02, 7.5540e-03,  ..., 1.0285e-02,\n           3.3629e-02, 6.2326e-02],\n          ...,\n          [3.0307e-01, 1.3352e-02, 1.1130e-01,  ..., 1.1427e-01,\n           1.3485e-01, 2.6162e-01],\n          [5.8355e-01, 8.7555e-02, 1.8269e-02,  ..., 1.2252e-02,\n           4.1085e-02, 1.1239e-01],\n          [4.6727e-01, 5.4180e-02, 4.4834e-02,  ..., 1.9202e-02,\n           8.2529e-02, 1.7813e-01]],\n\n         ...,\n\n         [[7.0418e-01, 9.3052e-03, 2.6017e-02,  ..., 5.0585e-03,\n           3.1567e-02, 2.0269e-01],\n          [1.6567e-06, 4.4776e-08, 1.0000e+00,  ..., 7.1404e-07,\n           1.6723e-08, 1.7373e-10],\n          [1.4283e-06, 1.9742e-06, 1.3319e-05,  ..., 2.8335e-09,\n           4.7592e-08, 3.3798e-07],\n          ...,\n          [2.4357e-06, 2.4195e-13, 3.6181e-07,  ..., 1.7095e-06,\n           9.9999e-01, 3.5307e-06],\n          [1.3493e-03, 1.6829e-07, 6.5037e-10,  ..., 6.6989e-07,\n           4.2350e-04, 9.9821e-01],\n          [9.9373e-01, 1.4930e-06, 6.6953e-05,  ..., 1.0104e-03,\n           7.2966e-04, 4.3300e-03]],\n\n         [[5.5196e-01, 4.2577e-02, 2.7362e-02,  ..., 5.0241e-02,\n           4.5528e-02, 1.8297e-01],\n          [9.3028e-02, 5.9493e-01, 3.5596e-02,  ..., 1.2533e-02,\n           4.8772e-02, 2.1946e-02],\n          [6.8545e-01, 4.5669e-02, 1.2335e-02,  ..., 2.8144e-02,\n           3.0468e-02, 9.6294e-02],\n          ...,\n          [1.5926e-01, 2.1363e-01, 1.5851e-02,  ..., 1.7765e-01,\n           6.0981e-02, 1.1803e-01],\n          [6.2725e-01, 6.9080e-02, 1.6177e-02,  ..., 3.4321e-02,\n           2.1225e-02, 1.1475e-01],\n          [7.1480e-01, 2.4511e-02, 2.0966e-02,  ..., 2.1037e-02,\n           4.3943e-02, 1.0124e-01]],\n\n         [[9.4219e-01, 1.3371e-03, 2.1412e-03,  ..., 4.4139e-03,\n           1.1041e-02, 2.6393e-02],\n          [4.3112e-01, 9.4139e-03, 5.3087e-02,  ..., 4.7938e-02,\n           3.1076e-02, 5.5605e-02],\n          [2.4033e-01, 5.0898e-02, 4.0780e-02,  ..., 7.7743e-02,\n           1.2406e-01, 8.9063e-02],\n          ...,\n          [5.9836e-02, 9.9195e-02, 2.2909e-02,  ..., 8.0521e-02,\n           3.7660e-02, 5.3550e-02],\n          [6.3541e-01, 1.7035e-02, 1.2118e-02,  ..., 3.6441e-02,\n           6.3955e-02, 1.4316e-01],\n          [9.8362e-01, 3.4177e-04, 1.1950e-03,  ..., 1.5438e-03,\n           3.4105e-03, 7.0925e-03]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.1260e-01, 4.0908e-04, 2.5672e-03,  ..., 5.6230e-04,\n           2.4641e-02, 6.5207e-01],\n          [1.6755e-01, 1.2475e-01, 2.2279e-03,  ..., 3.4570e-05,\n           1.2305e-02, 6.9215e-01],\n          [1.5540e-01, 1.4944e-04, 5.2318e-02,  ..., 1.5172e-04,\n           2.8977e-02, 7.6113e-01],\n          ...,\n          [4.0470e-01, 1.3311e-05, 1.7477e-03,  ..., 4.2199e-03,\n           5.2698e-03, 5.8392e-01],\n          [3.5926e-01, 5.9774e-05, 5.8546e-03,  ..., 7.9415e-05,\n           3.6690e-02, 5.9545e-01],\n          [2.4162e-01, 1.3985e-03, 5.6904e-03,  ..., 3.3338e-03,\n           2.0439e-02, 7.1212e-01]],\n\n         [[3.9243e-01, 8.2994e-03, 8.1206e-03,  ..., 3.0389e-03,\n           5.5427e-02, 5.0409e-01],\n          [2.3396e-01, 1.6145e-02, 4.4339e-02,  ..., 5.1717e-02,\n           7.5456e-02, 3.6223e-01],\n          [2.0530e-01, 8.3365e-02, 5.5191e-02,  ..., 4.6262e-02,\n           2.6528e-02, 2.7316e-01],\n          ...,\n          [1.1881e-01, 2.2220e-01, 4.0330e-02,  ..., 2.4507e-02,\n           6.6041e-02, 1.5056e-01],\n          [6.5330e-02, 2.9336e-02, 1.8202e-02,  ..., 1.2533e-02,\n           3.2744e-02, 7.8244e-01],\n          [4.5371e-01, 1.9566e-03, 1.6762e-02,  ..., 1.0033e-03,\n           2.3466e-02, 4.9297e-01]],\n\n         [[1.0666e-01, 7.3513e-02, 1.0125e-01,  ..., 5.1667e-02,\n           7.6050e-02, 2.0794e-01],\n          [2.3036e-01, 3.6349e-03, 1.9113e-01,  ..., 1.4433e-02,\n           2.5963e-01, 2.5570e-01],\n          [1.9741e-01, 1.0366e-03, 3.9325e-01,  ..., 1.0735e-03,\n           2.6804e-01, 1.3359e-01],\n          ...,\n          [2.7983e-01, 1.4190e-02, 6.5145e-02,  ..., 3.8559e-02,\n           7.0378e-02, 4.7195e-01],\n          [2.6178e-01, 4.2886e-03, 1.5647e-01,  ..., 3.7103e-03,\n           2.9270e-01, 2.5842e-01],\n          [1.2435e-01, 1.2056e-02, 9.5054e-03,  ..., 8.3018e-03,\n           9.6168e-03, 7.7100e-01]],\n\n         ...,\n\n         [[7.8878e-01, 5.0293e-03, 2.3711e-02,  ..., 5.2122e-03,\n           3.3004e-02, 1.2040e-01],\n          [2.6826e-01, 3.6225e-03, 6.3246e-01,  ..., 1.8054e-03,\n           5.1623e-03, 5.6591e-02],\n          [5.2974e-02, 1.2261e-02, 6.6108e-03,  ..., 7.1753e-03,\n           2.6678e-03, 1.9614e-01],\n          ...,\n          [5.1164e-01, 6.2811e-05, 2.4929e-03,  ..., 6.8583e-03,\n           3.5912e-01, 1.0907e-01],\n          [7.3334e-01, 1.7218e-03, 1.2963e-04,  ..., 1.5645e-03,\n           2.9850e-02, 2.2442e-01],\n          [8.5953e-01, 3.3004e-03, 9.3042e-03,  ..., 1.8279e-03,\n           3.2195e-02, 7.6201e-02]],\n\n         [[3.1264e-01, 8.5473e-03, 2.6883e-03,  ..., 6.1872e-03,\n           7.0159e-03, 6.4485e-01],\n          [3.1044e-01, 3.4992e-03, 1.1989e-02,  ..., 1.3980e-02,\n           1.0547e-02, 5.9816e-01],\n          [1.0707e-01, 1.8297e-01, 8.2877e-03,  ..., 1.3403e-01,\n           1.0382e-02, 3.6485e-01],\n          ...,\n          [2.9528e-01, 2.6024e-02, 3.6082e-02,  ..., 4.3312e-02,\n           7.0967e-02, 2.1843e-01],\n          [1.0956e-01, 1.9885e-01, 2.1935e-02,  ..., 1.5062e-01,\n           2.6919e-02, 1.6628e-01],\n          [2.7834e-01, 2.4522e-03, 1.0701e-03,  ..., 7.1135e-04,\n           3.5643e-03, 7.0583e-01]],\n\n         [[7.2850e-01, 1.5097e-02, 5.3808e-03,  ..., 9.2440e-03,\n           4.8736e-02, 1.7087e-01],\n          [7.0553e-01, 1.9486e-03, 6.6666e-03,  ..., 2.1789e-03,\n           2.8231e-02, 2.2935e-01],\n          [5.4621e-01, 1.0236e-01, 2.5970e-02,  ..., 1.2879e-03,\n           8.0408e-03, 2.7577e-01],\n          ...,\n          [3.8497e-02, 3.6997e-02, 2.3321e-02,  ..., 1.3353e-02,\n           2.2737e-02, 7.8782e-02],\n          [1.3517e-01, 3.2973e-02, 1.8381e-02,  ..., 1.3305e-01,\n           1.0935e-01, 2.6719e-01],\n          [8.1345e-01, 5.0982e-03, 2.1587e-03,  ..., 2.1636e-03,\n           1.1983e-02, 1.5810e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[6.2157e-02, 1.0512e-02, 9.1985e-03,  ..., 1.3984e-02,\n           2.2992e-02, 8.5325e-01],\n          [2.3143e-02, 5.7863e-02, 1.0307e-02,  ..., 5.9910e-02,\n           2.2650e-02, 7.7003e-01],\n          [1.5885e-02, 2.6412e-01, 2.7757e-03,  ..., 1.9340e-02,\n           2.4082e-02, 5.7946e-01],\n          ...,\n          [2.1836e-02, 5.5026e-02, 5.9443e-03,  ..., 2.2292e-01,\n           5.1496e-03, 3.4463e-01],\n          [4.1827e-02, 5.2039e-02, 1.0601e-01,  ..., 1.7217e-02,\n           1.0672e-01, 5.8068e-01],\n          [7.0564e-02, 7.2845e-04, 7.2425e-03,  ..., 1.0854e-03,\n           1.4753e-01, 7.6915e-01]],\n\n         [[2.2432e-01, 5.6599e-02, 1.5342e-02,  ..., 1.6377e-01,\n           7.6734e-02, 1.0841e-02],\n          [4.1822e-03, 1.2180e-01, 7.0786e-02,  ..., 1.6406e-01,\n           3.3330e-02, 1.4076e-01],\n          [1.4387e-02, 9.2236e-02, 1.6107e-01,  ..., 1.4109e-01,\n           1.6249e-01, 4.7362e-02],\n          ...,\n          [3.0424e-03, 6.9357e-02, 6.0040e-02,  ..., 2.3211e-02,\n           1.3610e-02, 4.4810e-01],\n          [1.2586e-02, 8.5993e-02, 8.0992e-02,  ..., 3.4911e-01,\n           1.1750e-01, 1.1874e-02],\n          [5.3086e-02, 3.0919e-02, 2.9400e-02,  ..., 5.4775e-02,\n           2.6552e-02, 6.6927e-01]],\n\n         [[1.3870e-01, 1.0063e-01, 1.0817e-02,  ..., 1.0408e-01,\n           6.7102e-02, 1.9558e-01],\n          [4.1527e-02, 8.1008e-03, 1.1215e-02,  ..., 3.4759e-02,\n           3.6747e-02, 8.1584e-01],\n          [5.1175e-02, 1.4644e-01, 3.9870e-02,  ..., 5.0199e-02,\n           1.0157e-01, 3.7749e-01],\n          ...,\n          [1.0242e-01, 3.9391e-03, 8.3824e-03,  ..., 1.5857e-02,\n           3.3504e-02, 7.3326e-01],\n          [1.5432e-01, 9.3620e-02, 1.8621e-02,  ..., 2.0251e-02,\n           6.3301e-02, 1.2577e-01],\n          [1.0752e-01, 3.4314e-03, 8.1964e-03,  ..., 5.8886e-03,\n           2.3297e-02, 8.2567e-01]],\n\n         ...,\n\n         [[2.5435e-02, 1.5318e-01, 3.6988e-02,  ..., 2.3225e-02,\n           5.4153e-03, 5.7656e-01],\n          [1.2303e-01, 1.0127e-02, 8.3013e-03,  ..., 5.6738e-02,\n           4.6354e-02, 4.1168e-01],\n          [1.3751e-01, 2.7699e-02, 1.4168e-01,  ..., 4.7499e-03,\n           4.6516e-02, 3.8952e-01],\n          ...,\n          [9.8416e-02, 4.6611e-02, 1.8714e-02,  ..., 3.6439e-02,\n           8.1765e-02, 4.6059e-01],\n          [8.2174e-02, 1.0346e-01, 2.1967e-01,  ..., 1.9327e-02,\n           6.2024e-02, 3.5420e-01],\n          [3.4939e-02, 4.7564e-03, 1.9738e-03,  ..., 3.7098e-03,\n           9.0538e-03, 9.1753e-01]],\n\n         [[1.8179e-02, 6.9720e-03, 3.2590e-03,  ..., 1.8544e-03,\n           1.6323e-02, 9.3249e-01],\n          [2.3461e-02, 9.2585e-03, 3.6752e-03,  ..., 3.1503e-03,\n           1.8179e-02, 9.2867e-01],\n          [7.7708e-02, 5.6972e-01, 2.1124e-02,  ..., 2.2062e-03,\n           9.6159e-03, 2.6794e-01],\n          ...,\n          [9.4915e-02, 1.6805e-02, 2.4538e-02,  ..., 3.2595e-02,\n           1.9135e-02, 2.2700e-01],\n          [4.5812e-02, 1.7264e-01, 2.6414e-02,  ..., 1.5893e-01,\n           2.7966e-02, 3.8653e-02],\n          [1.1952e-02, 1.5772e-03, 9.3307e-04,  ..., 1.9256e-03,\n           7.3066e-03, 9.6618e-01]],\n\n         [[6.5565e-02, 3.5001e-01, 2.8479e-02,  ..., 1.3316e-02,\n           9.1416e-02, 3.7011e-01],\n          [2.6650e-02, 2.2351e-02, 3.5920e-01,  ..., 4.2269e-03,\n           5.8510e-03, 4.5039e-01],\n          [3.7033e-02, 7.3906e-02, 2.6515e-02,  ..., 1.2247e-03,\n           1.6776e-03, 5.8556e-01],\n          ...,\n          [1.8900e-02, 1.0276e-03, 9.7270e-04,  ..., 1.6474e-02,\n           2.0036e-01, 7.5276e-01],\n          [2.9258e-02, 3.0956e-01, 5.9319e-04,  ..., 2.3705e-03,\n           9.6908e-02, 5.4602e-01],\n          [1.5841e-02, 3.9175e-03, 3.6893e-03,  ..., 9.9955e-03,\n           1.2697e-02, 9.2973e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.4480e-02, 1.6487e-02, 3.6651e-02,  ..., 1.0964e-02,\n           1.8256e-02, 8.2641e-01],\n          [4.0872e-02, 2.3546e-02, 4.0560e-02,  ..., 1.6500e-02,\n           1.7351e-02, 6.6849e-01],\n          [4.9204e-02, 1.8848e-02, 2.1062e-02,  ..., 2.2885e-02,\n           2.6111e-02, 3.2064e-01],\n          ...,\n          [1.6912e-02, 1.3718e-03, 3.4943e-04,  ..., 2.5945e-02,\n           8.4042e-02, 8.4923e-01],\n          [8.0493e-02, 1.8607e-02, 4.4632e-03,  ..., 3.9004e-02,\n           1.7269e-01, 6.5846e-01],\n          [1.8583e-02, 2.1115e-03, 2.2118e-03,  ..., 5.4709e-03,\n           1.2708e-02, 9.4521e-01]],\n\n         [[1.8439e-02, 4.8001e-02, 6.0109e-03,  ..., 2.5028e-02,\n           1.2144e-02, 8.1084e-01],\n          [1.4610e-03, 3.3772e-03, 2.1953e-03,  ..., 3.6184e-05,\n           8.6225e-04, 9.8973e-01],\n          [3.8931e-02, 7.6797e-01, 2.1256e-02,  ..., 2.8734e-04,\n           2.3478e-03, 1.6234e-01],\n          ...,\n          [1.6655e-02, 3.6009e-03, 1.5417e-03,  ..., 1.1433e-02,\n           1.3692e-02, 9.3641e-01],\n          [2.5348e-02, 6.0499e-03, 9.3223e-04,  ..., 8.9650e-02,\n           4.9355e-02, 7.6010e-01],\n          [7.0102e-03, 3.8143e-03, 8.2139e-04,  ..., 1.6636e-03,\n           4.1435e-03, 9.7057e-01]],\n\n         [[3.4016e-02, 2.9721e-03, 9.7922e-03,  ..., 4.3546e-03,\n           2.4438e-02, 9.1259e-01],\n          [4.8503e-02, 1.2569e-02, 9.5898e-02,  ..., 4.5667e-02,\n           8.4134e-02, 5.4811e-01],\n          [1.2993e-01, 2.4679e-02, 5.0132e-02,  ..., 2.9855e-02,\n           1.2613e-01, 3.9388e-01],\n          ...,\n          [3.4636e-02, 4.2266e-02, 2.3227e-02,  ..., 5.6828e-02,\n           3.1406e-02, 6.7125e-01],\n          [1.7807e-01, 3.4102e-02, 1.4653e-01,  ..., 5.9061e-02,\n           1.0001e-01, 2.7644e-01],\n          [1.8301e-02, 2.9659e-03, 1.1068e-02,  ..., 1.4466e-03,\n           3.8819e-03, 9.5834e-01]],\n\n         ...,\n\n         [[6.5539e-02, 5.4356e-02, 1.1853e-02,  ..., 8.7543e-03,\n           1.1938e-01, 6.9044e-01],\n          [8.3105e-03, 2.6974e-04, 9.2212e-01,  ..., 2.4890e-04,\n           1.7987e-03, 5.9602e-02],\n          [1.3110e-03, 1.8587e-02, 3.3992e-03,  ..., 1.2568e-03,\n           1.4270e-04, 5.9713e-02],\n          ...,\n          [3.8483e-02, 9.0315e-05, 5.7534e-04,  ..., 2.5100e-03,\n           2.8948e-01, 6.6447e-01],\n          [1.9450e-02, 4.2985e-03, 2.2295e-04,  ..., 7.1985e-03,\n           1.6838e-01, 7.9050e-01],\n          [2.1971e-01, 1.6237e-03, 9.7288e-03,  ..., 1.2681e-02,\n           5.6424e-02, 6.8310e-01]],\n\n         [[2.9024e-02, 2.8299e-02, 1.3588e-02,  ..., 3.3100e-02,\n           7.0164e-02, 7.3305e-01],\n          [2.5699e-01, 5.3549e-03, 1.7261e-03,  ..., 1.7163e-03,\n           1.2154e-02, 7.1199e-01],\n          [5.9920e-02, 5.7282e-01, 2.8771e-02,  ..., 5.8730e-03,\n           1.9439e-03, 2.9810e-01],\n          ...,\n          [1.4391e-02, 4.1509e-03, 3.3274e-03,  ..., 6.3489e-03,\n           1.2674e-02, 1.5257e-01],\n          [2.0088e-02, 9.4441e-03, 2.4986e-03,  ..., 1.8952e-01,\n           1.2596e-01, 3.9008e-01],\n          [1.6687e-02, 2.4547e-03, 5.3429e-03,  ..., 3.4101e-03,\n           1.6529e-02, 9.2691e-01]],\n\n         [[3.9319e-02, 7.3647e-03, 7.5644e-03,  ..., 5.9961e-03,\n           4.4177e-02, 8.4637e-01],\n          [1.4768e-01, 2.4488e-02, 1.0266e-01,  ..., 7.8018e-03,\n           8.1735e-02, 4.4480e-01],\n          [1.5392e-01, 9.0503e-03, 5.0877e-02,  ..., 1.9977e-02,\n           1.4224e-01, 4.3138e-01],\n          ...,\n          [1.3541e-02, 3.3620e-03, 8.2242e-04,  ..., 1.4511e-02,\n           1.2671e-02, 8.9227e-01],\n          [6.2195e-02, 1.4279e-02, 1.0545e-02,  ..., 4.9327e-03,\n           7.5571e-02, 7.9505e-01],\n          [1.2660e-02, 6.4672e-03, 4.8099e-03,  ..., 9.6538e-03,\n           1.0962e-02, 9.3827e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.4231e-03, 3.9025e-01, 1.8721e-01,  ..., 8.9925e-02,\n           3.1665e-02, 3.1700e-02],\n          [1.2266e-03, 3.2206e-03, 1.1689e-03,  ..., 1.4801e-03,\n           5.9879e-04, 9.8888e-01],\n          [1.4994e-03, 1.9435e-03, 7.9954e-03,  ..., 5.4097e-03,\n           1.9343e-03, 9.6792e-01],\n          ...,\n          [2.9519e-03, 1.6047e-02, 1.6798e-03,  ..., 3.9205e-02,\n           1.8280e-03, 9.0702e-01],\n          [1.0346e-02, 1.8945e-01, 1.7060e-01,  ..., 4.9149e-02,\n           2.9330e-02, 1.7257e-01],\n          [3.1565e-03, 3.3692e-03, 1.4931e-03,  ..., 2.7820e-03,\n           3.3654e-03, 9.8122e-01]],\n\n         [[2.8294e-02, 4.1514e-01, 1.6895e-01,  ..., 1.7959e-02,\n           3.5037e-02, 1.5961e-01],\n          [7.6119e-02, 4.5075e-02, 7.9784e-02,  ..., 1.3966e-02,\n           6.2219e-02, 6.6772e-01],\n          [1.1345e-02, 3.8094e-01, 4.3786e-02,  ..., 1.3592e-02,\n           2.2765e-02, 4.8624e-01],\n          ...,\n          [4.9349e-03, 5.8765e-02, 1.9662e-02,  ..., 3.6816e-02,\n           7.7520e-03, 8.2349e-01],\n          [3.2416e-03, 4.0244e-01, 3.9478e-01,  ..., 5.6042e-03,\n           1.4446e-02, 5.4792e-02],\n          [8.6082e-03, 1.7559e-02, 6.3660e-03,  ..., 9.0510e-03,\n           6.7251e-03, 9.4233e-01]],\n\n         [[2.1533e-03, 2.3691e-02, 1.3111e-02,  ..., 4.3772e-03,\n           8.6265e-01, 3.7419e-02],\n          [7.0494e-02, 1.2270e-02, 4.2724e-03,  ..., 3.4396e-03,\n           1.1358e-02, 8.8802e-01],\n          [7.3375e-02, 5.7969e-02, 8.2542e-02,  ..., 2.3106e-03,\n           4.0053e-02, 7.1463e-01],\n          ...,\n          [5.0085e-02, 3.2062e-02, 4.7210e-02,  ..., 6.9766e-02,\n           5.0237e-02, 4.5738e-01],\n          [2.7183e-02, 1.0873e-01, 1.0998e-01,  ..., 4.4717e-02,\n           8.0475e-02, 1.7770e-01],\n          [1.9841e-02, 1.1071e-02, 6.7311e-03,  ..., 7.4330e-03,\n           3.3086e-02, 9.0026e-01]],\n\n         ...,\n\n         [[6.2856e-02, 3.1968e-03, 4.9507e-03,  ..., 7.8192e-03,\n           9.3586e-02, 7.9443e-01],\n          [8.5131e-02, 1.2830e-02, 5.2034e-03,  ..., 1.0209e-03,\n           3.9135e-02, 8.5035e-01],\n          [1.2566e-01, 7.4776e-02, 2.0097e-02,  ..., 1.4034e-03,\n           1.7279e-02, 7.4981e-01],\n          ...,\n          [4.9340e-02, 2.6181e-02, 1.1442e-02,  ..., 6.2303e-02,\n           3.9684e-02, 1.0490e-01],\n          [2.2498e-02, 2.5379e-03, 3.5086e-03,  ..., 4.4087e-02,\n           8.1459e-02, 6.4811e-01],\n          [1.4909e-02, 5.2490e-03, 6.2668e-03,  ..., 5.6073e-03,\n           2.6881e-02, 9.0737e-01]],\n\n         [[5.6354e-03, 4.1895e-02, 7.4022e-03,  ..., 4.2418e-02,\n           1.0796e-02, 4.6859e-01],\n          [3.5858e-03, 3.6083e-03, 9.9047e-04,  ..., 4.7583e-03,\n           2.0213e-02, 9.6190e-01],\n          [8.7189e-03, 1.1873e-02, 4.4930e-03,  ..., 3.3201e-03,\n           9.8467e-03, 9.4389e-01],\n          ...,\n          [5.9718e-03, 1.1135e-02, 3.6910e-03,  ..., 6.9842e-03,\n           7.4553e-03, 1.4640e-01],\n          [1.5099e-02, 3.6467e-02, 1.1584e-02,  ..., 4.1053e-02,\n           3.5808e-02, 5.1282e-01],\n          [8.2697e-03, 9.1773e-03, 6.3033e-03,  ..., 8.2917e-03,\n           1.2524e-02, 8.8835e-01]],\n\n         [[4.1983e-03, 3.9717e-02, 2.9497e-03,  ..., 1.0363e-02,\n           8.6217e-01, 5.3547e-02],\n          [4.0357e-02, 1.4987e-03, 1.1500e-03,  ..., 4.5613e-05,\n           4.8468e-03, 9.5127e-01],\n          [8.2769e-04, 9.5791e-01, 1.8963e-03,  ..., 3.9375e-04,\n           1.0763e-04, 3.8313e-02],\n          ...,\n          [3.4459e-04, 4.1481e-05, 1.0073e-04,  ..., 2.8625e-04,\n           3.1620e-03, 1.6746e-02],\n          [4.5979e-04, 2.5507e-03, 1.5275e-04,  ..., 2.2021e-01,\n           2.5049e-01, 4.9939e-01],\n          [5.7739e-03, 7.7936e-03, 3.5880e-03,  ..., 1.0899e-02,\n           2.1314e-02, 9.1062e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.2720e-03, 1.7889e-01, 8.1418e-02,  ..., 2.4075e-02,\n           4.3245e-02, 5.8458e-01],\n          [2.6633e-02, 4.4615e-02, 1.9187e-02,  ..., 7.5774e-03,\n           3.2689e-02, 8.5222e-01],\n          [7.6410e-02, 2.1800e-01, 1.0193e-01,  ..., 4.8228e-03,\n           5.8889e-02, 5.1869e-01],\n          ...,\n          [3.3764e-02, 2.8155e-01, 1.1547e-01,  ..., 3.4346e-02,\n           4.1082e-02, 3.1102e-01],\n          [1.1611e-02, 3.5331e-01, 1.7974e-01,  ..., 1.3508e-02,\n           7.1654e-02, 2.6886e-01],\n          [1.0277e-02, 7.9060e-03, 8.8001e-03,  ..., 2.5733e-03,\n           2.7719e-02, 9.3184e-01]],\n\n         [[7.2343e-02, 1.3203e-01, 1.4369e-01,  ..., 1.5930e-02,\n           7.1560e-02, 4.5055e-01],\n          [2.1192e-02, 9.6570e-02, 8.3852e-02,  ..., 3.3569e-02,\n           1.2308e-02, 4.6829e-01],\n          [2.8111e-02, 1.6538e-02, 3.9415e-02,  ..., 8.7424e-02,\n           2.6205e-02, 2.9224e-01],\n          ...,\n          [1.0095e-02, 5.5806e-03, 4.5693e-03,  ..., 8.4852e-03,\n           9.2701e-03, 9.4237e-01],\n          [3.9257e-02, 7.1242e-02, 7.0713e-02,  ..., 4.3385e-02,\n           6.9699e-02, 5.3791e-01],\n          [9.0110e-03, 2.9471e-03, 4.2073e-03,  ..., 3.6759e-03,\n           4.4320e-03, 9.6549e-01]],\n\n         [[4.9949e-02, 2.5219e-01, 1.8497e-02,  ..., 7.8481e-03,\n           2.4806e-02, 6.2929e-01],\n          [7.0375e-03, 1.9685e-02, 8.0039e-01,  ..., 1.6436e-03,\n           2.7054e-03, 1.3351e-01],\n          [1.0707e-02, 9.2921e-02, 3.3646e-02,  ..., 1.9264e-03,\n           1.0979e-03, 7.4270e-01],\n          ...,\n          [3.7053e-02, 1.8041e-02, 8.3454e-03,  ..., 4.0450e-03,\n           5.1682e-02, 8.3536e-01],\n          [1.2327e-01, 2.0212e-01, 1.0378e-02,  ..., 1.0108e-02,\n           3.1357e-02, 5.9219e-01],\n          [2.0316e-02, 9.9791e-03, 8.7038e-03,  ..., 9.8538e-03,\n           1.5864e-02, 9.0665e-01]],\n\n         ...,\n\n         [[7.6016e-03, 1.7147e-02, 5.3722e-03,  ..., 2.7113e-02,\n           1.6875e-02, 8.9220e-01],\n          [1.7510e-02, 3.6431e-02, 5.0198e-03,  ..., 8.8274e-03,\n           4.0840e-02, 8.5930e-01],\n          [1.1763e-01, 1.2626e-01, 7.5314e-03,  ..., 8.1851e-03,\n           3.9337e-02, 6.7533e-01],\n          ...,\n          [2.1579e-02, 4.1616e-02, 1.0823e-02,  ..., 4.9315e-02,\n           1.0878e-02, 3.2039e-01],\n          [9.7064e-03, 1.3674e-02, 4.9291e-03,  ..., 4.9092e-02,\n           1.0188e-01, 7.1547e-01],\n          [1.4706e-02, 6.6095e-03, 4.6854e-03,  ..., 9.3769e-03,\n           3.0460e-02, 9.1137e-01]],\n\n         [[1.2052e-02, 2.9430e-02, 5.4344e-03,  ..., 1.3923e-01,\n           5.1540e-03, 7.6831e-01],\n          [9.6395e-03, 6.8820e-02, 2.0972e-02,  ..., 2.5081e-02,\n           4.5420e-03, 7.5660e-01],\n          [1.5955e-02, 5.6418e-02, 1.0326e-02,  ..., 1.6557e-02,\n           1.7715e-02, 7.4405e-01],\n          ...,\n          [4.1004e-03, 3.8191e-03, 4.0481e-04,  ..., 1.6126e-03,\n           2.0476e-03, 9.8050e-01],\n          [5.6124e-02, 7.6145e-02, 8.8778e-03,  ..., 5.9784e-02,\n           4.4120e-02, 6.6265e-01],\n          [1.4250e-02, 5.4502e-03, 1.9655e-03,  ..., 3.4344e-03,\n           6.8163e-03, 9.5629e-01]],\n\n         [[1.2329e-03, 6.1729e-03, 6.6829e-04,  ..., 1.5827e-03,\n           9.6430e-01, 2.2419e-02],\n          [2.2898e-02, 7.2585e-02, 1.3820e-02,  ..., 1.3267e-03,\n           8.0419e-04, 8.8161e-01],\n          [5.1122e-03, 7.4936e-01, 7.2875e-03,  ..., 2.8447e-04,\n           8.4298e-05, 2.3225e-01],\n          ...,\n          [7.4787e-03, 3.2668e-03, 2.1833e-04,  ..., 5.5763e-03,\n           4.1042e-03, 9.6677e-01],\n          [2.5786e-02, 7.1007e-02, 1.9134e-03,  ..., 3.0264e-02,\n           7.3849e-02, 7.3786e-01],\n          [1.5523e-02, 5.8430e-03, 1.4163e-03,  ..., 6.2278e-03,\n           4.0557e-02, 9.2198e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[5.7287e-02, 3.3338e-02, 6.2609e-02,  ..., 5.2324e-03,\n           1.6933e-02, 7.3536e-01],\n          [1.7271e-01, 4.1198e-02, 5.0306e-02,  ..., 2.9687e-02,\n           2.6577e-02, 4.1857e-01],\n          [1.1634e-01, 2.4077e-02, 4.7584e-02,  ..., 1.8643e-02,\n           5.0980e-02, 4.7613e-01],\n          ...,\n          [2.3119e-01, 4.1721e-03, 3.1456e-03,  ..., 1.5641e-02,\n           8.0477e-03, 7.0286e-01],\n          [1.8681e-01, 1.8737e-02, 2.4629e-02,  ..., 4.9050e-03,\n           3.1679e-02, 6.8757e-01],\n          [3.0507e-02, 6.1578e-03, 6.5536e-03,  ..., 2.6886e-03,\n           5.3761e-03, 9.3722e-01]],\n\n         [[3.7964e-02, 1.7524e-01, 7.7539e-02,  ..., 3.3227e-02,\n           2.2045e-02, 4.5445e-01],\n          [3.2447e-02, 3.0096e-02, 1.7851e-02,  ..., 2.5485e-02,\n           1.1347e-01, 7.2038e-01],\n          [7.3060e-02, 2.9341e-02, 6.3268e-02,  ..., 1.0104e-02,\n           1.4620e-01, 6.4963e-01],\n          ...,\n          [3.0071e-02, 2.2861e-02, 9.5608e-03,  ..., 5.9507e-02,\n           4.2388e-02, 8.0107e-01],\n          [5.4935e-02, 9.4375e-02, 5.3809e-02,  ..., 1.9389e-02,\n           5.8235e-02, 6.7523e-01],\n          [1.4169e-02, 1.6938e-02, 1.5405e-02,  ..., 1.8276e-02,\n           6.0563e-02, 8.2447e-01]],\n\n         [[9.0027e-03, 1.6552e-01, 2.1347e-03,  ..., 9.0275e-04,\n           3.5082e-03, 8.1207e-01],\n          [1.9914e-03, 9.0591e-03, 9.7166e-02,  ..., 7.3257e-04,\n           1.3891e-03, 8.7262e-01],\n          [1.1072e-03, 3.4324e-02, 4.3286e-03,  ..., 6.8165e-04,\n           3.2940e-04, 9.3234e-01],\n          ...,\n          [2.0785e-02, 6.8600e-03, 1.1162e-03,  ..., 1.7571e-03,\n           1.8531e-02, 9.4847e-01],\n          [2.1104e-03, 4.4569e-01, 1.7296e-03,  ..., 7.7875e-04,\n           1.3962e-03, 5.3515e-01],\n          [1.2675e-02, 1.1139e-02, 5.6444e-03,  ..., 7.9009e-03,\n           8.0219e-03, 9.3790e-01]],\n\n         ...,\n\n         [[1.4994e-02, 3.4254e-02, 1.0177e-02,  ..., 4.5366e-02,\n           5.8754e-03, 8.5856e-01],\n          [9.1402e-02, 2.8398e-02, 6.6219e-02,  ..., 7.3424e-02,\n           3.9662e-02, 4.8546e-01],\n          [2.0314e-01, 1.7617e-02, 7.4246e-03,  ..., 5.5011e-02,\n           9.3962e-02, 4.6955e-01],\n          ...,\n          [2.0051e-01, 6.6820e-02, 2.9608e-02,  ..., 2.3783e-02,\n           6.8214e-02, 3.9834e-01],\n          [7.9026e-02, 5.3515e-02, 7.0020e-03,  ..., 3.4055e-02,\n           1.2067e-02, 7.1038e-01],\n          [2.3408e-02, 1.6942e-02, 6.9135e-03,  ..., 9.9531e-03,\n           2.1366e-02, 8.9801e-01]],\n\n         [[2.0314e-03, 1.8162e-03, 4.6716e-04,  ..., 9.7500e-03,\n           1.9938e-03, 9.7849e-01],\n          [1.5026e-02, 3.0281e-02, 3.1912e-02,  ..., 5.4830e-02,\n           2.5005e-02, 6.9205e-01],\n          [1.2804e-02, 1.6203e-01, 3.1092e-02,  ..., 8.1880e-02,\n           3.1199e-02, 5.7778e-01],\n          ...,\n          [1.7849e-02, 4.9762e-02, 6.0223e-03,  ..., 4.1761e-02,\n           1.7653e-02, 7.0170e-01],\n          [5.4749e-03, 2.8177e-02, 7.5994e-03,  ..., 1.3942e-02,\n           1.0306e-02, 9.0715e-01],\n          [2.3226e-03, 5.9737e-03, 1.9143e-03,  ..., 8.6651e-03,\n           3.1451e-03, 9.7091e-01]],\n\n         [[2.4066e-02, 1.5262e-02, 1.4096e-02,  ..., 7.8373e-03,\n           3.4650e-02, 8.9180e-01],\n          [6.0730e-02, 8.8636e-02, 5.7785e-02,  ..., 5.8118e-02,\n           8.3530e-02, 5.9778e-01],\n          [5.4098e-02, 2.8829e-01, 4.8276e-02,  ..., 3.2653e-02,\n           1.2865e-01, 4.1797e-01],\n          ...,\n          [7.6533e-02, 1.2315e-01, 4.8085e-02,  ..., 2.1169e-02,\n           3.8179e-02, 3.6938e-01],\n          [5.6276e-02, 1.4810e-01, 4.6869e-02,  ..., 2.5700e-02,\n           7.8068e-02, 5.9049e-01],\n          [1.3104e-02, 2.3136e-02, 1.1072e-02,  ..., 1.5643e-02,\n           2.6471e-02, 8.9051e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.1056e-02, 1.5406e-01, 1.0242e-01,  ..., 3.8051e-02,\n           4.6693e-02, 4.3708e-01],\n          [8.2798e-03, 2.1616e-01, 2.5927e-02,  ..., 2.0651e-02,\n           9.1474e-03, 6.6951e-01],\n          [1.0831e-02, 2.2809e-01, 9.7924e-02,  ..., 1.5562e-02,\n           1.2123e-02, 5.9907e-01],\n          ...,\n          [4.5205e-03, 1.9198e-02, 2.1877e-03,  ..., 1.2308e-01,\n           1.2150e-02, 7.5480e-01],\n          [1.2199e-02, 5.1039e-02, 1.7235e-02,  ..., 5.4235e-03,\n           1.1679e-02, 8.7139e-01],\n          [3.4494e-03, 3.5587e-02, 6.3773e-03,  ..., 1.8756e-02,\n           8.5054e-04, 9.0371e-01]],\n\n         [[1.3270e-02, 2.5524e-02, 1.4384e-02,  ..., 5.0088e-02,\n           4.8020e-02, 7.5119e-01],\n          [2.0648e-01, 3.3104e-02, 1.8526e-02,  ..., 2.8175e-02,\n           1.1383e-02, 6.1696e-01],\n          [4.2694e-01, 1.4300e-02, 6.2756e-03,  ..., 8.6213e-03,\n           2.7450e-02, 4.8661e-01],\n          ...,\n          [1.0917e-01, 5.1108e-02, 9.2009e-03,  ..., 1.7266e-02,\n           1.4333e-02, 7.7742e-01],\n          [1.9076e-01, 1.4987e-02, 1.3532e-02,  ..., 2.7556e-02,\n           1.0869e-02, 6.7476e-01],\n          [1.0060e-01, 4.7428e-02, 4.0696e-02,  ..., 3.4299e-02,\n           1.4553e-02, 7.2389e-01]],\n\n         [[3.1180e-02, 2.9166e-02, 2.0490e-02,  ..., 1.1746e-02,\n           3.6835e-02, 7.8303e-01],\n          [9.2532e-02, 7.9640e-02, 1.0353e-01,  ..., 1.4351e-02,\n           6.3976e-02, 5.9497e-01],\n          [6.5456e-02, 1.6792e-01, 1.2845e-01,  ..., 1.0102e-02,\n           6.2394e-02, 5.0633e-01],\n          ...,\n          [6.6457e-02, 1.4641e-01, 4.9287e-02,  ..., 5.6603e-02,\n           2.8198e-02, 4.7055e-01],\n          [1.8864e-02, 7.9084e-02, 5.2567e-02,  ..., 3.0944e-02,\n           2.0006e-02, 5.5572e-01],\n          [3.1262e-02, 1.5495e-01, 8.8651e-02,  ..., 1.3283e-02,\n           5.1152e-02, 5.1280e-01]],\n\n         ...,\n\n         [[1.7466e-02, 2.2394e-02, 1.7059e-02,  ..., 7.0618e-03,\n           1.0397e-02, 8.7471e-01],\n          [8.1209e-02, 3.1120e-02, 7.0288e-02,  ..., 2.0524e-02,\n           3.9515e-02, 7.2387e-01],\n          [2.6880e-02, 1.0812e-01, 6.4670e-03,  ..., 4.7610e-03,\n           9.6566e-03, 8.1631e-01],\n          ...,\n          [7.1919e-03, 5.1960e-03, 2.1972e-03,  ..., 1.4321e-02,\n           4.0350e-03, 9.2516e-01],\n          [1.1202e-02, 1.4227e-02, 1.4525e-03,  ..., 9.7683e-03,\n           1.0339e-02, 9.1842e-01],\n          [4.4563e-02, 5.1184e-02, 3.4788e-02,  ..., 3.0206e-02,\n           5.0341e-02, 7.1864e-01]],\n\n         [[4.8672e-02, 1.7246e-01, 1.3767e-01,  ..., 2.9014e-02,\n           4.6674e-02, 3.4495e-01],\n          [1.8564e-01, 8.1266e-02, 3.4008e-02,  ..., 5.3685e-02,\n           1.1452e-01, 2.0298e-01],\n          [2.0174e-01, 1.9841e-01, 2.2217e-02,  ..., 4.0396e-02,\n           1.3156e-01, 1.1551e-01],\n          ...,\n          [2.3351e-01, 5.0253e-02, 1.0453e-02,  ..., 2.0542e-02,\n           1.3047e-02, 5.8086e-01],\n          [1.6555e-01, 1.0096e-02, 6.6250e-03,  ..., 1.0512e-02,\n           3.8899e-03, 7.7932e-01],\n          [1.1080e-01, 1.6039e-01, 4.5829e-02,  ..., 3.0059e-02,\n           2.8587e-02, 4.4723e-01]],\n\n         [[9.0445e-02, 5.7287e-02, 7.3817e-02,  ..., 5.2536e-02,\n           1.0263e-01, 2.9482e-01],\n          [1.6506e-01, 2.8707e-03, 1.2253e-02,  ..., 1.3363e-02,\n           6.7967e-02, 6.3924e-01],\n          [4.4624e-01, 3.1660e-02, 7.1012e-02,  ..., 1.0507e-02,\n           1.1643e-01, 2.5618e-01],\n          ...,\n          [4.2780e-02, 3.7484e-02, 1.5688e-02,  ..., 2.1416e-02,\n           2.9882e-02, 6.6697e-01],\n          [7.0547e-02, 4.0872e-02, 5.0853e-02,  ..., 4.3658e-02,\n           9.6572e-02, 6.3929e-01],\n          [1.4526e-01, 8.1402e-02, 1.0088e-01,  ..., 7.0310e-02,\n           2.1327e-01, 1.1040e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.2557e-02, 1.7331e-01, 7.1381e-02,  ..., 4.3068e-02,\n           1.2742e-01, 1.5138e-01],\n          [4.4802e-02, 3.3381e-01, 2.9370e-02,  ..., 1.6019e-02,\n           2.5772e-01, 2.5558e-01],\n          [8.2646e-02, 3.7296e-01, 4.4945e-02,  ..., 1.0951e-02,\n           1.8073e-01, 2.2096e-01],\n          ...,\n          [1.1320e-02, 5.3119e-02, 2.0755e-02,  ..., 7.4933e-02,\n           4.3401e-01, 3.1578e-01],\n          [2.1017e-02, 3.4514e-02, 1.5027e-02,  ..., 5.5088e-02,\n           4.0495e-01, 4.0447e-01],\n          [2.5339e-02, 3.6245e-02, 1.5373e-02,  ..., 5.3331e-02,\n           3.9036e-01, 4.1993e-01]],\n\n         [[1.4400e-02, 1.7715e-01, 1.0790e-02,  ..., 3.2296e-02,\n           3.5950e-01, 2.5696e-01],\n          [8.4425e-04, 6.5291e-02, 2.4046e-03,  ..., 4.3453e-03,\n           5.1352e-01, 4.0990e-01],\n          [6.1538e-03, 2.7215e-02, 1.3865e-02,  ..., 7.2020e-03,\n           4.8553e-01, 4.5465e-01],\n          ...,\n          [6.5359e-04, 2.4638e-03, 2.6232e-04,  ..., 6.3068e-01,\n           1.8934e-01, 1.7152e-01],\n          [2.8354e-03, 2.7189e-03, 9.9554e-04,  ..., 1.3127e-03,\n           5.5844e-01, 4.3089e-01],\n          [2.4046e-03, 2.7127e-03, 7.7272e-04,  ..., 1.2498e-03,\n           5.1601e-01, 4.7450e-01]],\n\n         [[1.1352e-02, 2.0316e-02, 2.2928e-02,  ..., 3.4369e-02,\n           4.8585e-01, 3.8530e-01],\n          [8.3094e-02, 2.7388e-01, 1.1494e-01,  ..., 5.8546e-03,\n           2.0212e-01, 2.7706e-01],\n          [1.1648e-01, 1.2767e-01, 1.5981e-01,  ..., 6.3692e-03,\n           1.9893e-01, 2.6803e-01],\n          ...,\n          [1.4784e-01, 4.5006e-03, 6.2552e-03,  ..., 1.7829e-01,\n           2.5209e-01, 1.7657e-01],\n          [1.9136e-02, 1.1309e-02, 1.0727e-02,  ..., 1.3769e-02,\n           5.5085e-01, 3.5938e-01],\n          [2.8456e-02, 2.0676e-02, 1.7235e-02,  ..., 2.1320e-02,\n           4.4834e-01, 4.1967e-01]],\n\n         ...,\n\n         [[1.5345e-02, 4.0295e-02, 1.7518e-02,  ..., 1.3110e-02,\n           3.9290e-01, 5.0945e-01],\n          [3.5745e-04, 7.6678e-02, 4.0362e-03,  ..., 3.0001e-03,\n           4.2308e-01, 4.8785e-01],\n          [2.3718e-03, 2.4384e-02, 2.1047e-01,  ..., 3.8213e-03,\n           3.5046e-01, 3.9454e-01],\n          ...,\n          [2.3768e-03, 4.7948e-03, 8.9300e-04,  ..., 1.5942e-01,\n           3.8286e-01, 4.4359e-01],\n          [7.6476e-04, 4.7129e-03, 3.3071e-03,  ..., 2.6650e-03,\n           5.3077e-01, 4.5351e-01],\n          [9.6090e-04, 4.9871e-03, 2.7604e-03,  ..., 2.5611e-03,\n           4.8790e-01, 4.9706e-01]],\n\n         [[5.9340e-03, 2.4557e-02, 5.2366e-03,  ..., 4.9754e-03,\n           5.7153e-01, 3.2068e-01],\n          [8.2293e-01, 1.2378e-01, 4.6437e-03,  ..., 4.9099e-05,\n           3.9640e-03, 3.5061e-02],\n          [9.4195e-01, 1.6796e-02, 2.0963e-02,  ..., 2.9215e-05,\n           1.8493e-03, 1.3299e-02],\n          ...,\n          [1.7724e-01, 4.2076e-04, 6.1563e-05,  ..., 7.3960e-01,\n           7.2956e-03, 4.5590e-02],\n          [1.4387e-01, 1.1693e-01, 7.1781e-02,  ..., 5.4905e-02,\n           2.0699e-01, 2.2723e-01],\n          [1.5823e-01, 1.1934e-01, 6.9965e-02,  ..., 3.9070e-02,\n           2.1673e-01, 2.4938e-01]],\n\n         [[5.7884e-02, 2.7724e-01, 4.0855e-02,  ..., 8.4480e-02,\n           7.2612e-02, 9.3853e-02],\n          [2.0822e-03, 1.1029e-01, 1.4432e-01,  ..., 1.2730e-01,\n           1.2700e-01, 9.6964e-02],\n          [2.2073e-03, 2.2079e-01, 8.6932e-02,  ..., 2.4997e-01,\n           1.4986e-01, 1.3416e-01],\n          ...,\n          [7.6626e-03, 2.1674e-02, 4.7697e-02,  ..., 1.3262e-02,\n           5.4176e-01, 3.3349e-01],\n          [6.6987e-02, 1.9920e-02, 8.7407e-03,  ..., 2.2659e-02,\n           4.6808e-01, 3.7045e-01],\n          [5.0699e-02, 2.5875e-02, 9.8597e-03,  ..., 2.4640e-02,\n           4.4526e-01, 4.0925e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.1865, 0.0836, 0.0485,  ..., 0.0307, 0.2877, 0.2956],\n          [0.0560, 0.0531, 0.0243,  ..., 0.0173, 0.4071, 0.4108],\n          [0.0536, 0.0959, 0.0388,  ..., 0.0132, 0.3899, 0.3849],\n          ...,\n          [0.0206, 0.0083, 0.0040,  ..., 0.0141, 0.4777, 0.4573],\n          [0.0100, 0.0033, 0.0043,  ..., 0.0023, 0.4951, 0.4803],\n          [0.0100, 0.0032, 0.0042,  ..., 0.0022, 0.4952, 0.4805]],\n\n         [[0.2111, 0.0587, 0.0440,  ..., 0.1617, 0.1576, 0.1630],\n          [0.0749, 0.1164, 0.0293,  ..., 0.0324, 0.3156, 0.3136],\n          [0.0516, 0.0632, 0.0147,  ..., 0.0167, 0.3878, 0.3884],\n          ...,\n          [0.0612, 0.1015, 0.0368,  ..., 0.0393, 0.2816, 0.2843],\n          [0.0079, 0.0083, 0.0066,  ..., 0.0042, 0.4886, 0.4687],\n          [0.0078, 0.0079, 0.0062,  ..., 0.0041, 0.4892, 0.4699]],\n\n         [[0.1281, 0.1877, 0.0449,  ..., 0.1456, 0.0462, 0.0475],\n          [0.0197, 0.0692, 0.0250,  ..., 0.0506, 0.2970, 0.2960],\n          [0.0378, 0.1003, 0.0752,  ..., 0.0600, 0.1268, 0.1266],\n          ...,\n          [0.0323, 0.0206, 0.0131,  ..., 0.1947, 0.2742, 0.2759],\n          [0.0136, 0.0077, 0.0067,  ..., 0.0078, 0.4749, 0.4683],\n          [0.0134, 0.0079, 0.0070,  ..., 0.0082, 0.4740, 0.4678]],\n\n         ...,\n\n         [[0.0810, 0.1263, 0.1295,  ..., 0.1607, 0.0116, 0.0119],\n          [0.0660, 0.0764, 0.0365,  ..., 0.0337, 0.2518, 0.2428],\n          [0.1247, 0.0353, 0.1362,  ..., 0.0337, 0.1231, 0.1204],\n          ...,\n          [0.0648, 0.0216, 0.0566,  ..., 0.1455, 0.2082, 0.2059],\n          [0.0271, 0.0065, 0.0058,  ..., 0.0123, 0.4757, 0.4590],\n          [0.0286, 0.0068, 0.0062,  ..., 0.0129, 0.4733, 0.4579]],\n\n         [[0.0393, 0.2352, 0.0609,  ..., 0.0930, 0.2368, 0.2487],\n          [0.0353, 0.0420, 0.0097,  ..., 0.0598, 0.4026, 0.3856],\n          [0.0293, 0.0153, 0.0059,  ..., 0.0259, 0.4513, 0.4442],\n          ...,\n          [0.0148, 0.0408, 0.0149,  ..., 0.0268, 0.4052, 0.3967],\n          [0.0103, 0.0063, 0.0036,  ..., 0.0060, 0.4851, 0.4693],\n          [0.0104, 0.0063, 0.0036,  ..., 0.0059, 0.4849, 0.4703]],\n\n         [[0.0069, 0.0227, 0.0101,  ..., 0.1430, 0.3822, 0.3879],\n          [0.0082, 0.0454, 0.0282,  ..., 0.0479, 0.4227, 0.4044],\n          [0.0097, 0.0165, 0.0101,  ..., 0.0374, 0.4602, 0.4457],\n          ...,\n          [0.0219, 0.0200, 0.0119,  ..., 0.1493, 0.3568, 0.3506],\n          [0.0041, 0.0055, 0.0030,  ..., 0.0079, 0.4980, 0.4723],\n          [0.0040, 0.0053, 0.0029,  ..., 0.0080, 0.4977, 0.4731]]]],\n       grad_fn=<SoftmaxBackward0>)))"},"metadata":{}}]},{"cell_type":"code","source":"outputs[:2]","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:09:56.643918Z","iopub.execute_input":"2024-02-01T01:09:56.644345Z","iopub.status.idle":"2024-02-01T01:09:56.653947Z","shell.execute_reply.started":"2024-02-01T01:09:56.644317Z","shell.execute_reply":"2024-02-01T01:09:56.652649Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(tensor(0.3689, grad_fn=<NllLossBackward0>),\n tensor([[-0.2489,  0.5581]], grad_fn=<AddmmBackward0>))"},"metadata":{}}]},{"cell_type":"code","source":"outputs = model(**inputs,\n                labels=labels,\n                output_hidden_states=True,\n                output_attentions=True,)\ntuple_out = outputs.to_tuple()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T01:12:46.990397Z","iopub.execute_input":"2024-02-01T01:12:46.990783Z","iopub.status.idle":"2024-02-01T01:12:47.080869Z","shell.execute_reply.started":"2024-02-01T01:12:46.990757Z","shell.execute_reply":"2024-02-01T01:12:47.079831Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}