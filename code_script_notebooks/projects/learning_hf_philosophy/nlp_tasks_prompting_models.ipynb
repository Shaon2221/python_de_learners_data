{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import pipeline\nimport torch\n\ntorch.manual_seed(0)\n# decoder for text generation or causalLM work\ngenerator = pipeline('text-generation', model = 'gpt2')\nprompt = \"Hello, I'm a language model\"\n\ngenerator(prompt, max_length = 30)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T12:14:21.992173Z","iopub.execute_input":"2024-02-06T12:14:21.992590Z","iopub.status.idle":"2024-02-06T12:14:58.785166Z","shell.execute_reply.started":"2024-02-06T12:14:21.992554Z","shell.execute_reply":"2024-02-06T12:14:58.783401Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-06 12:14:32.350808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-06 12:14:32.351003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-06 12:14:32.548805: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3062b5e0c9aa4e4cabe72949b212f791"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2286971a68024a748c14575b3ef5dca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eaf1ffb53454197bcf8af4d2ee5937f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8562708895b347ecad348ef3fdfd6687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae61942a519423c85f01eb2e4c09437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644f9035cf0b47fe810c2d1a6ac35a88"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"Hello, I'm a language model\\n\\nYou're just like me.\\n\\nThis is why I keep saying,\\n\\nIt's like,\"}]"},"metadata":{}}]},{"cell_type":"code","source":"# encoder decoder for text2text generation\n\ntext2text_generator = pipeline(\"text2text-generation\", \n                               model = 'google/flan-t5-base')\nprompt = \"Translate from English to French: I'm very happy to see you\"\n\ntext2text_generator(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:15:38.024127Z","iopub.execute_input":"2024-02-06T12:15:38.024962Z","iopub.status.idle":"2024-02-06T12:15:46.012133Z","shell.execute_reply.started":"2024-02-06T12:15:38.024921Z","shell.execute_reply":"2024-02-06T12:15:46.010710Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91dc3cab2cec4c708e7b4cacd8877c0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"446af20b7a5b4dfaab720e29436816dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9bd35ee62e741f39a962d1776c30768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc445bac5824982a7698cf4331daa6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19a396840b749c1bb8462dc28eeba43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecccc86988dd4418b5dfb5f1bf351810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aac7f76ed174e2395fbb8d51a21e897"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1128: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': 'Je suis trÃ¨s heureuse de vous rencontrer.'}]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer\nimport torch\n\ntorch.manual_seed(0)\n\nmodel = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:16:33.053435Z","iopub.execute_input":"2024-02-06T12:16:33.053863Z","iopub.status.idle":"2024-02-06T12:17:57.352919Z","shell.execute_reply.started":"2024-02-06T12:16:33.053831Z","shell.execute_reply":"2024-02-06T12:17:57.351311Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3cc55ed6999414ea24f349a4ad6e4b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d0534942ac4147b35d2c9abc9f8ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aa021597b814b26ae102adbb4f7a8cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba3ee27d6d7493a9c1313dcb04cb2d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52d7529af124d87bfae520e6eb91f71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd920f88465c46e28fd4ddffd0d1d501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a3718d4155448f08408500aaf0dbd86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6cc0c1874404c6dab48168ce22aa58d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d611b86c73b14b1c996e396537b37aa3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7f7fb1e5e84476bb34fc504a9ed05b"}},"metadata":{}}]},{"cell_type":"code","source":"pipe.device","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:18:34.463509Z","iopub.execute_input":"2024-02-06T12:18:34.464043Z","iopub.status.idle":"2024-02-06T12:18:34.475392Z","shell.execute_reply.started":"2024-02-06T12:18:34.463999Z","shell.execute_reply":"2024-02-06T12:18:34.474030Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"# text classification\n\nprompt = \"\"\"Classify the text into neutral, negative or positive. \nText: This movie is definitely one of my favorite movies of its kind. \nThe interaction between respectable and morally strong characters is an \node to chivalry and the honor code amongst thieves and policemen.\nSentiment:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=10,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:19:31.427525Z","iopub.execute_input":"2024-02-06T12:19:31.427992Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Named Entity Recognition (NER) is a task of finding named entities \n# in a piece of text, such as a person, location, or organization. \n\nprompt = \"\"\"Return a list of named entities in the text.\nText: The Golden State Warriors are an American professional basketball team based in San Francisco.\nNamed entities:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=15,\n    return_full_text = False,    \n)\n\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# translation\n\nprompt = \"\"\"Translate the English text to Italian.\nText: Sometimes, I've believed as many as six impossible things before breakfast.\nTranslation:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=20,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text summarization is another generative task where the output heavily relies on the\n# input, and encoder-decoder models can be a better choice\n\nprompt = \"\"\"Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.\nWrite a summary of the above text.\nSummary:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=30,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question Answering\n# question answering task we can structure the prompt into the following \n# logical components: instructions, context, question, and the leading \n# word or phrase (\"Answer:\")\n\nprompt = \"\"\"Answer the question using the context below.\nContext: Gazpacho is a cold soup and drink made of raw, blended vegetables. Most gazpacho includes stale bread, tomato, cucumbers, onion, bell peppers, garlic, olive oil, wine vinegar, water, and salt. Northern recipes often include cumin and/or pimentÃ³n (smoked sweet paprika). Traditionally, gazpacho was made by pounding the vegetables in a mortar with a pestle; this more laborious method is still sometimes used as it helps keep the gazpacho cool and avoids the foam and silky consistency of smoothie versions made in blenders or food processors.\nQuestion: What modern tool is used to make gazpacho?\nAnswer:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=10,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reasoning is one of the most difficult tasks for LLMs, and achieving good results \n# often requires applying advanced prompting techniques, \n\ntorch.manual_seed(5)\nprompt = \"\"\"There are 5 groups of students in the class. Each group has 4 students. \nHow many students are there in the class?\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=30,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"I baked 15 muffins. I ate 2 muffins and gave 5 muffins to a neighbor. My partner then bought 6 more muffins and ate 2. How many muffins do we now have?\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=10,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}