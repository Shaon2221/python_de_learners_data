{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrate-Search-Predict (DSP), with 'Py'thon\n",
    "\n",
    "- Developed by SNLPG team, show the list  \n",
    "\n",
    "- DSPy provides general purpose modules that replace string-based \n",
    "  prompting. DSPy also provides optimizers which can optimize these \n",
    "  modules with help of following\n",
    "\n",
    "- What are the various modules DSPy has?\n",
    "  Lang Models, Signatures, Modules, Data, Metrics, Optimizers, Assertions\n",
    "\n",
    "- Can you talk about use cases of DSPy?\n",
    "  QA, Classification, Summarisatio, RAGs / Multi-Hop rags, \n",
    "  Reasoning\n",
    "\n",
    "- Can we see a basic example of the DSPy code?\n",
    "    Has a LM, Signature, Module and execution. \n",
    "    Code follows below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Example of DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('D:\\\\gitFolders\\\\python_de_learners_data\\\\.env')\n",
    "import openai\n",
    "import os\n",
    "from dspy import (\n",
    "    Signature,\n",
    "    OpenAI,\n",
    "    Predict,\n",
    "    settings\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo  = OpenAI(model='gpt-3.5-turbo',)\n",
    "settings.configure(lm=turbo)\n",
    "# settings.configure(turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! How can I assist you today?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can access the LM directly as below\n",
    "turbo(\"Hi there, I am using DSPY...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best way is to use the Modules \n",
    "pred_mod = Predict('question -> answer')\n",
    "model_out = pred_mod(question='WHere is Lunar landing happend the first time?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Question: Where is Lunar landing happened the first time?\\nAnswer: The first Lunar landing happened on the Moon.'\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is Lunar landing happened the first time?\n",
      "Answer: The first Lunar landing happened on the Moon.\n"
     ]
    }
   ],
   "source": [
    "print(model_out.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: WHere is Lunar landing happend the first time?\\nAnswer:',\n",
       "  'response': {'id': 'chatcmpl-8yvKkXsAN5lNoKIrXwpL5P9j23jd2',\n",
       "   'choices': [{'finish_reason': 'stop',\n",
       "     'index': 0,\n",
       "     'logprobs': None,\n",
       "     'message': {'content': 'Question: Where is Lunar landing happened the first time?\\nAnswer: The first Lunar landing happened on the Moon.',\n",
       "      'role': 'assistant',\n",
       "      'function_call': None,\n",
       "      'tool_calls': None}}],\n",
       "   'created': 1709530762,\n",
       "   'model': 'gpt-3.5-turbo-0125',\n",
       "   'object': 'chat.completion',\n",
       "   'system_fingerprint': 'fp_2b778c6b35',\n",
       "   'usage': {'completion_tokens': 22,\n",
       "    'prompt_tokens': 51,\n",
       "    'total_tokens': 73}},\n",
       "  'kwargs': {'stringify_request': '{\"temperature\": 0.0, \"max_tokens\": 150, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"model\": \"gpt-3.5-turbo\", \"messages\": [{\"role\": \"user\", \"content\": \"Given the fields `question`, produce the fields `answer`.\\\\n\\\\n---\\\\n\\\\nFollow the following format.\\\\n\\\\nQuestion: ${question}\\\\nAnswer: ${answer}\\\\n\\\\n---\\\\n\\\\nQuestion: WHere is Lunar landing happend the first time?\\\\nAnswer:\"}]}'},\n",
       "  'raw_kwargs': {}},\n",
       " {'prompt': 'Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: WHere is Lunar landing happend the first time?\\nAnswer:',\n",
       "  'response': {'id': 'chatcmpl-8yvKkXsAN5lNoKIrXwpL5P9j23jd2',\n",
       "   'choices': [{'finish_reason': 'stop',\n",
       "     'index': 0,\n",
       "     'logprobs': None,\n",
       "     'message': {'content': 'Question: Where is Lunar landing happened the first time?\\nAnswer: The first Lunar landing happened on the Moon.',\n",
       "      'role': 'assistant',\n",
       "      'function_call': None,\n",
       "      'tool_calls': None}}],\n",
       "   'created': 1709530762,\n",
       "   'model': 'gpt-3.5-turbo-0125',\n",
       "   'object': 'chat.completion',\n",
       "   'system_fingerprint': 'fp_2b778c6b35',\n",
       "   'usage': {'completion_tokens': 22,\n",
       "    'prompt_tokens': 51,\n",
       "    'total_tokens': 73}},\n",
       "  'kwargs': {'stringify_request': '{\"temperature\": 0.0, \"max_tokens\": 150, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"model\": \"gpt-3.5-turbo\", \"messages\": [{\"role\": \"user\", \"content\": \"Given the fields `question`, produce the fields `answer`.\\\\n\\\\n---\\\\n\\\\nFollow the following format.\\\\n\\\\nQuestion: ${question}\\\\nAnswer: ${answer}\\\\n\\\\n---\\\\n\\\\nQuestion: WHere is Lunar landing happend the first time?\\\\nAnswer:\"}]}'},\n",
       "  'raw_kwargs': {}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbo.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: WHere is Lunar landing happend the first time?\n",
      "Answer:\u001b[32m Question: Where is Lunar landing happened the first time?\n",
      "Answer: The first Lunar landing happened on the Moon.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But can you tell me Why DSPy?\n",
    "\n",
    "- Streamline the process of prompt-engineering with input and objective \n",
    "\n",
    "- Create Modules of LMs that can work to achieve objective \n",
    "\n",
    "- Then train the modules by using Datasets built by hand, and with LLMs \n",
    "\n",
    "- Evalute the modules with the Metrics that is custom coded in DSPy\n",
    "\n",
    "- Including Fine-tuning smaller models like T5 and Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you Tell me how to use Open Source Model Integration with Dspy \n",
    "\n",
    "- Can you tell me How to configure LMs in DSPy \n",
    "  OpenAI, HF_Models, Ollama_models\n",
    "\n",
    "- Can you show some tasks we can perform with these models?\n",
    "    Creating a minimal code example to show how the models \n",
    "perform on QA, Classification and Summarisation tasks\n",
    "\n",
    "##### Before that Why?\n",
    "\n",
    "- DSPy process makes it possible to use smaller, cheaper model by the way of Optimizing the Modules\n",
    "\n",
    "- Depending the tasks, the LMs can be fine-tuned using the DSPy objects also.\n",
    "\n",
    "- Learn about supporting application like HuggingFace Hub, Ollama, TGI etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/aicoder/gitfolder/python_de_learners_data/code_script_notebooks/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import (\n",
    "    OpenAI,\n",
    "    OllamaLocal,\n",
    "    HFModel,\n",
    "    Predict,\n",
    "    Signature,\n",
    "    settings,\n",
    "    context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = OpenAI(model='gpt-3.5-turbo')\n",
    "settings.configure(lm=turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only a single Predict is required\n",
    "pred_qa = Predict('question -> answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response by OpenAI:  Question: Who is the world's fastest man and woman?\n",
      "Answer: The world's fastest man is Usain Bolt from Jamaica, and the world's fastest woman is Florence Griffith-Joyner from the United States.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the world fastest man and woman\"\n",
    "\n",
    "with context(lm=turbo):\n",
    "    resp = pred_qa(question=question)\n",
    "    print(\"Response by OpenAI: \", resp.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First lets check the ollama python interface\n",
    "mistral_ollama = OllamaLocal(model='mistralq4',\n",
    "                            model_type='text',\n",
    "                            base_url='http://localhost:11434',\n",
    "                            temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral Response:  [INSTR]\n",
      "  Given a question, produce the answer by looking up the answer in a database.\n",
      "[/INSTR]\n",
      "\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "with context(lm=mistral_ollama):\n",
    "    resp = pred_qa(question=question)\n",
    "    print(\"Mistral Response: \", resp.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_ollama = OllamaLocal(model='gemma:2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Response:  Prediction(\n",
      "    answer=\"Question: Who is the world fastest man and woman\\nAnswer: This context does not provide any information about the world's fastest man and woman, so I cannot generate the requested fields from the context.\"\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with context(lm=gemma_ollama):\n",
    "    resp = pred_qa(question=question)\n",
    "    print(\"Gemma Response: \", resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3914b69df674988b3460b6f2bc7bc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets load HF Model\n",
    "gemma_hf = HFModel(\"google/gemma-2b-it\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"As of October 26, 2023, the world's fastest man is **Usain Bolt**, with a recorded speed of 10.31 seconds in the 200 meters race.\\n\\nThe world's fastest woman is **Florence Griffith-Joyner**, with a recorded speed of 11.34 seconds in the 100 meters race.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_ollama(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/aimachine/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma HF Response:  Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Who is the world fastest man and woman\n",
      "Answer: N/A\n"
     ]
    }
   ],
   "source": [
    "with context(lm=gemma_hf):\n",
    "    resp = pred_qa(question=question)\n",
    "    print(\"Gemma HF Response: \", resp.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turbo Multi Response:  Question: Who is the world fastest man and woman\n",
      "Answer: The world's fastest man is Usain Bolt from Jamaica, with a record time of 9.58 seconds in the 100m dash. The world's fastest woman is Florence Griffith-Joyner from the United States, with a record time of 10.49 seconds in the 100m dash.\n",
      "\n",
      "\n",
      "Turbo Multi Response:  Question: Who is the world's fastest man and woman?\n",
      "Answer: The world's fastest man is Usain Bolt from Jamaica, and the world's fastest woman is Florence Griffith-Joyner from the United States.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_pred_qa = Predict('question -> answer', n=2)\n",
    "\n",
    "with context(lm=turbo):\n",
    "    resp = n_pred_qa(question=question)\n",
    "    for resp_elem in resp.completions:\n",
    "        print(\"Turbo Multi Response: \", resp_elem.answer)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are other clients present\n",
    "\n",
    "- HFClientTGI\n",
    "\n",
    "- HFClientVLLM\n",
    "\n",
    "- ChatModuleClient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
