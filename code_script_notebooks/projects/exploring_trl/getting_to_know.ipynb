{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Reward: The primary goal is to maximize the reward achieved by the model during RL training. \n",
    "\n",
    "Objective KL Divergence: KL divergence (Kullback-Leibler divergence) measures the dissimilarity between two probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model. \n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "# This is going to be 2nd model, only for reference\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "# Common tokenizer \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring on the trainer\n",
    "ppo_config = {\"batch_size\": 1,\n",
    "              \"learning_rate\": 1e-5}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n",
    "# trainer takes the config, two models and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt = \"This morning I went to the \"\n",
    "query_tensor = tokenizer.encode(query_txt, return_tensors='pt').to(model.pretrained_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token,\n",
    "    \"max_new_tokens\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tensor = ppo_trainer.generate([item for item in query_tensor],\n",
    "                                       return_prompt=False,\n",
    "                                       **gen_kwargs)\n",
    "\n",
    "response_text = tokenizer.decode(response_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the reward and train loop...\n",
    "reward = [torch.tensor(1.0, device=model.pretrained_model.device)]\n",
    "\n",
    "# 6. train model with ppo\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the model on the Hub\n",
    "model.push_to_hub(\"fine_tune_gpt_ppo\")\n",
    "\n",
    "# or save it locally\n",
    "model.save_pretrained(\"fine_tune_gpt_ppo\")\n",
    "\n",
    "# load the model from the Hub, the model is loaded with AutoModel directly\n",
    "# not the TRL class\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"my-fine-tuned-model-ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create optimizer\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "adam_optimizer = bnb.optim.Adam8Bit(model.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO_trainers with different optimizers\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(sgd_optimizer, gamma=0.9)\n",
    "updated_trainer_sgd = PPOTrainer(ppo_config, model,\n",
    "                                 model_ref, tokenizer,\n",
    "                                 optimizer=sgd_optimizer,\n",
    "                                 lr_scheduler=lr_scheduler)\n",
    "\n",
    "updated_trainer_adam = PPOTrainer(ppo_config, model, model_ref, tokenizer, optimizer=adam_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
