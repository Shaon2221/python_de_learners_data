{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-22T14:49:04.611833Z","iopub.execute_input":"2024-02-22T14:49:04.612214Z","iopub.status.idle":"2024-02-22T14:49:22.086888Z","shell.execute_reply.started":"2024-02-22T14:49:04.612172Z","shell.execute_reply":"2024-02-22T14:49:22.085373Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"One can easily fine-tune your SFT model using SFTTrainer from TRL. Let us assume your dataset is imdb, the text you want to predict is inside the text field of the dataset, and you want to fine-tune the facebook/opt-350m model.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer\n\ndataset = load_dataset(\"imdb\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:49:29.677325Z","iopub.execute_input":"2024-02-22T14:49:29.677753Z","iopub.status.idle":"2024-02-22T14:50:01.682146Z","shell.execute_reply.started":"2024-02-22T14:49:29.677716Z","shell.execute_reply":"2024-02-22T14:50:01.680630Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-22 14:49:45.874912: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-22 14:49:45.875020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-22 14:49:46.161294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/__init__.py:23\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     is_bitsandbytes_available,\n\u001b[1;32m     10\u001b[0m     is_diffusers_available,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     is_xpu_available,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     AutoModelForCausalLMWithValueHead,\n\u001b[1;32m     18\u001b[0m     AutoModelForSeq2SeqLMWithValueHead,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     setup_chat_format,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     DataCollatorForCompletionOnlyLM,\n\u001b[1;32m     25\u001b[0m     DPOTrainer,\n\u001b[1;32m     26\u001b[0m     IterativeSFTTrainer,\n\u001b[1;32m     27\u001b[0m     ModelConfig,\n\u001b[1;32m     28\u001b[0m     PPOConfig,\n\u001b[1;32m     29\u001b[0m     PPOTrainer,\n\u001b[1;32m     30\u001b[0m     RewardConfig,\n\u001b[1;32m     31\u001b[0m     RewardTrainer,\n\u001b[1;32m     32\u001b[0m     SFTTrainer,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_kbit_device_map, get_peft_config, get_quantization_config\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_diffusers_available():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/__init__.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreward_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RewardConfig\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreward_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RewardTrainer, compute_accuracy\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msft_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PartialState\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaInferenceError\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetGenerationError\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     28\u001b[0m     AutoTokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     TrainingArguments,\n\u001b[1;32m     35\u001b[0m )\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (/opt/conda/lib/python3.10/site-packages/datasets/arrow_writer.py)"],"ename":"ImportError","evalue":"cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (/opt/conda/lib/python3.10/site-packages/datasets/arrow_writer.py)","output_type":"error"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n# may be a seperate training_arg object has to be passed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can use the DataCollatorForCompletionOnlyLM to train your model on the **generated prompts only**. Note that this works only in the case when packing=False. To instantiate that collator for instruction data, pass a response template and the tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n\ndataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\nresponse_template = \" ### Answer:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    formatting_func=formatting_prompts_func,\n    data_collator=collator,\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To instantiate that collator for assistant style conversation data, pass a response template, an instruction template and the tokenizer. Here is an example of how it would work to fine-tune opt-350m **on assistant completions** only on the Open Assistant Guanaco dataset:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instruction_template = \"### Human:\"\nresponse_template = \"### Assistant:\"\ncollator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n                                           response_template=response_template,\n                                           tokenizer=tokenizer,\n                                           mlm=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    data_collator=collator,\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\ndef print_tokens_with_ids(txt):\n    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n    print(list(zip(tokens, token_ids)))\n\nprompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\nprint_tokens_with_ids(prompt)  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n\nresponse_template = \"### Assistant:\"\nprint_tokens_with_ids(response_template) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **setup_chat_format() function** in trl easily sets up a model and tokenizer for conversational AI tasks. This function:\n\n- Adds special tokens to the tokenizer, e.g. <|im_start|> and <|im_end|>, to indicate the start and end of a conversation.\n\n- Resizes the model’s embedding layer to accommodate the new tokens.\n\n- Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n\n- optionally you can pass resize_to_multiple_of to resize the embedding layer to a multiple of the resize_to_multiple_of argument, e.g. 64. If you want to see more formats being supported in the future, please open a GitHub issue on trl","metadata":{}},{"cell_type":"code","source":"# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n\n# Set up the chat format with default 'chatml' format\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load jsonl dataset\ndataset = load_dataset(\"json\", data_files=\"path/to/dataset.jsonl\", split=\"train\")\n# load dataset from the HuggingFace Hub\ndataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    args=training_args,\n    train_dataset=dataset,\n    packing=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following is very powerful way of tackling dataset loading","metadata":{}},{"cell_type":"code","source":"def formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['question'])):\n        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n        output_texts.append(text)\n    return output_texts\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    formatting_func=formatting_prompts_func,\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SFTTrainer **supports example packing**, where multiple short examples are packed in the same input sequence to increase training efficiency. This is done with the ConstantLengthDataset utility class that returns constant length chunks of tokens from a stream of examples. \n\nTo enable the usage of this dataset class, simply pass packing=True to the SFTTrainer constructor.","metadata":{}},{"cell_type":"code","source":"def formatting_func(example):\n    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n    return text\n\ntrainer = SFTTrainer(\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    packing=True,\n    formatting_func=formatting_func\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ntrainer = SFTTrainer(\n    \"EleutherAI/gpt-neo-125m\",\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    peft_config=peft_config\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n# trainig adapter with 8-bit model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"EleutherAI/gpt-neo-125m\",\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    peft_config=peft_config,\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this enables use of flash_attention1\n\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use Flash Attention 2, first install the latest flash-attn package:\n\npip install flash_attention","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=True,\n    attn_implementation=\"flash_attention_2\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using model creation utility","metadata":{}},{"cell_type":"code","source":"from trl import ModelConfig, SFTTrainer, get_kbit_device_map, get_peft_config, get_quantization_config\n\nmodel_config = ModelConfig(\n    model_name_or_path=\"facebook/opt-350m\"\n    attn_implementation=None, # or \"flash_attention_2\"\n)\n\ntorch_dtype = (\n    model_config.torch_dtype\n    if model_config.torch_dtype in [\"auto\", None]\n    else getattr(torch, model_config.torch_dtype)\n)\n\nquantization_config = get_quantization_config(model_config)\n\nmodel_kwargs = dict(\n    revision=model_config.model_revision,\n    trust_remote_code=model_config.trust_remote_code,\n    attn_implementation=model_config.attn_implementation,\n    torch_dtype=torch_dtype,\n    use_cache=False if training_args.gradient_checkpointing else True,\n    device_map=get_kbit_device_map() if quantization_config is not None else None,\n    quantization_config=quantization_config,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_config.model_name_or_path,\n                                             **model_kwargs)\n\ntrainer = SFTTrainer(\n    ...,\n    model=model_config.model_name_or_path,\n    peft_config=get_peft_config(model_config),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reward Trainer","metadata":{}},{"cell_type":"markdown","source":"The reward model should be trained on a dataset of paired examples, where each example is a tuple of two sequences. The reward model should be trained to predict which example in the pair is more relevant to the task at hand.\n\nThe reward trainer expects a very specific format for the dataset. The dataset should contain two 4 entries at least if you don’t use the default RewardDataCollatorWithPadding data collator. \n\nTherefore the final dataset object should contain two 4 entries at least if you use \nthe default RewardDataCollatorWithPadding data collator. The entries should be named:\n\ninput_ids_chosen\n\nattention_mask_chosen\n\ninput_ids_rejected\n\nattention_mask_rejected\n\nYou should pass an **AutoModelForSequenceClassification model** to the RewardTrainer, along with a RewardConfig which configures the hyperparameters of the training.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom trl import RewardTrainer, RewardConfig\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\n...\n\ntrainer = RewardTrainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    peft_config=peft_config,\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]}]}