{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3c3337b-16c9-46f0-a69f-aac9a633dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\",\n",
    "                       split=\"train\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f4333-c714-4650-977f-3e16ea1f2c88",
   "metadata": {},
   "source": [
    "The **first step** is to train your SFT model (see the SFTTrainer), to ensure the data we train on is in-distribution for the PPO algorithm. \n",
    "\n",
    "In addition we need to train a Reward model (see RewardTrainer) which will be used to optimize the SFT model using the PPO algorithm\n",
    "\n",
    "**Objective:** The PPOTrainer expects to align a generated response with a query given the rewards obtained from the Reward model.\n",
    "\n",
    "During each step of the PPO algorithm \n",
    "\n",
    "- Sample a batch of prompts from the dataset\n",
    "\n",
    "```\n",
    "ppo_dataset_dict = {\n",
    "    \"query\": [\n",
    "        \"Explain the moon landing to a 6 year old in a few sentences.\",\n",
    "        \"Why arenâ€™t birds real?\",\n",
    "        \"What happens if you fire a cannonball directly at a pumpkin at high speeds?\",\n",
    "        \"How can I steal from a grocery store without getting caught?\",\n",
    "        \"Why is it important to eat socks after meditating? \"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "  \n",
    "- Use these prompts to generate the a responses from the SFT model.\n",
    "  \n",
    "- Reward model is used to compute the rewards for the generated response.\n",
    "  \n",
    "- Rewards are used to optimize the SFT model using the PPO algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14f46bb8-c526-4488-b68d-904d6ebe8c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'input_ids'],\n",
       "    num_rows: 16\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e848b4dc-7e6d-4e07-b659-03e7ed776316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4234dca4-c2ea-4239-b562-1709a280a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with=None, task_name=None, model_name='gpt2', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=128, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=1, world_size=1, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=False, is_peft_model=False, backward_batch_size=128, global_backward_batch_size=128, global_batch_size=128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af78ad30-144a-4712-a3e6-53e6fb54e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99514480-8f10-4e5c-b1a1-e975164f0686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec4ee31cc9b41348a698d4eeacbac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71290047-1b84-46c9-93dd-a3cea4dcf675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af37c6f3-32c6-4c39-b911-9a12899d2ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27400da3115441688efeea791ccf9282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e99af6e4f42649e45d31a4d1e8c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac61cf71be49a4b7a1c729fb033fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb702ce58c84ce6ae61248a8e99f50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7062663f0ce4f28b2fda77cd62fa70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a3fb0a9aa746129885b64e24392d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reward can be generated using any function that returns a single value for a string, \n",
    "# be it a simple rule (e.g. length of string), a metric (e.g. BLEU), or a reward model based on \n",
    "# human preferences.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea4785b-2358-4523-9977-733729d1bb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878938f65f1443aabca0fff984ffbac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd970c4-4151-4119-a445-337e63c8c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372cf091-bb9e-44ed-9b56-80a188304848",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/home/aicoder/training/sftt_opt',\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172d8cb-46ea-4bba-8841-a62d085af1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ref_model must be a PreTrainedModelWrapper or `None`, \n",
    "got <class 'transformers.pipelines.text_classification.TextClassificationPipeline'> - \n",
    "supported architectures are: (<class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>, \n",
    "<class 'trl.models.modeling_value_head.AutoModelForSeq2SeqLMWithValueHead'>)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5b9078a-5265-4e58-92d2-3c368fcde697",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(config,\n",
    "                         model,\n",
    "                         ref_model,\n",
    "                         tokenizer,\n",
    "                         dataset=dataset,\n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "befb6947-34b1-483f-92fb-7e2b5bf21c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12fb6c29-6a7f-4447-9971-6472a30a77f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/aicoder/aimachine/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "194it [17:33,  5.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# loop over the dataset\n",
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    \n",
    "    pipe_outputs = reward_model(texts, **sent_kwargs)\n",
    "    \n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d34add1a-94fb-46d4-8082-2584d623d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/aimachine/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Awful! Aw</td>\n",
       "      <td>Who I Remember falls off the ridge and</td>\n",
       "      <td>brilliant, beautiful adventure.\\n\\nIf</td>\n",
       "      <td>-2.447360</td>\n",
       "      <td>0.305777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vipul Shah has done some</td>\n",
       "      <td>exceptional work here as well. He has been mi...</td>\n",
       "      <td>of my best to capture. I want to thank you a ...</td>\n",
       "      <td>2.416112</td>\n",
       "      <td>2.226157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This has just been</td>\n",
       "      <td>ake, from Star Trek The Original Series</td>\n",
       "      <td>amazing to just use her comedy and it</td>\n",
       "      <td>0.482322</td>\n",
       "      <td>2.469725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Superb</td>\n",
       "      <td>more goodness straight from</td>\n",
       "      <td>I love the book</td>\n",
       "      <td>2.258950</td>\n",
       "      <td>2.798766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saw this film</td>\n",
       "      <td>he has any core belief and</td>\n",
       "      <td>positive writer himself and he likes</td>\n",
       "      <td>0.731313</td>\n",
       "      <td>2.230472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was fully</td>\n",
       "      <td>on the podcast.</td>\n",
       "      <td>and was able to</td>\n",
       "      <td>1.620422</td>\n",
       "      <td>2.346437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I've seen the</td>\n",
       "      <td>puppies. I've seen those who see</td>\n",
       "      <td>I am reminded of the great talks of</td>\n",
       "      <td>1.204584</td>\n",
       "      <td>2.414796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don</td>\n",
       "      <td>founders and other directors of the Toronto</td>\n",
       "      <td>very carefully. And I composer Sir</td>\n",
       "      <td>0.943619</td>\n",
       "      <td>1.515602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This movie wasn't just bad -</td>\n",
       "      <td>terrible. It was a violation of all the regul...</td>\n",
       "      <td>. I've loved it's got a terrific scale with</td>\n",
       "      <td>-2.524200</td>\n",
       "      <td>2.597310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Why is</td>\n",
       "      <td>so hard to save what you've saved? What happe...</td>\n",
       "      <td>beautiful. Mourning future of NPR's, unique a...</td>\n",
       "      <td>-1.273580</td>\n",
       "      <td>2.687011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;br /&gt;&lt;</td>\n",
       "      <td>&gt; &lt;/div&gt; {{#link \"fulltext</td>\n",
       "      <td>You loved the kernel and knew what is inspired by</td>\n",
       "      <td>-0.307524</td>\n",
       "      <td>1.923537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I caught</td>\n",
       "      <td>up w/ league members.</td>\n",
       "      <td>It's so very funny,</td>\n",
       "      <td>0.792709</td>\n",
       "      <td>2.661392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>This film had a</td>\n",
       "      <td>time I called Wilson's Mind</td>\n",
       "      <td>and popular actors were excellent us</td>\n",
       "      <td>1.315583</td>\n",
       "      <td>2.594474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I caught this movie</td>\n",
       "      <td>a Thursday night, and I remember it vividly. ...</td>\n",
       "      <td>Very nice movie. Choice movie. Fun was best m...</td>\n",
       "      <td>2.512933</td>\n",
       "      <td>2.591718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I got myself a copy of this</td>\n",
       "      <td>2 days ago.\"\\n\\nMy friends in town had</td>\n",
       "      <td>a great longsword, and it comes through with ...</td>\n",
       "      <td>0.749915</td>\n",
       "      <td>2.725136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Karen(</td>\n",
       "      <td>works well on cementing surfaces, sorry ;)</td>\n",
       "      <td>woman and proudly's contents. I heartily</td>\n",
       "      <td>-0.350012</td>\n",
       "      <td>1.905749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           query  ... rewards (after)\n",
       "0                      Awful! Aw  ...        0.305777\n",
       "1       Vipul Shah has done some  ...        2.226157\n",
       "2             This has just been  ...        2.469725\n",
       "3                         Superb  ...        2.798766\n",
       "4                  Saw this film  ...        2.230472\n",
       "5                    I was fully  ...        2.346437\n",
       "6                  I've seen the  ...        2.414796\n",
       "7                          I don  ...        1.515602\n",
       "8   This movie wasn't just bad -  ...        2.597310\n",
       "9                         Why is  ...        2.687011\n",
       "10                       <br /><  ...        1.923537\n",
       "11                      I caught  ...        2.661392\n",
       "12               This film had a  ...        2.594474\n",
       "13           I caught this movie  ...        2.591718\n",
       "14   I got myself a copy of this  ...        2.725136\n",
       "15                        Karen(  ...        1.905749\n",
       "\n",
       "[16 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in reward_model(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in reward_model(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da2e23de-672f-4b31-b778-b6b08acee787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.507862\n",
       "rewards (after)     2.249629\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.771312\n",
       "rewards (after)     2.442261\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bcdc5cf-ce0d-43f8-89c3-81c3d3a87768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/aicoder/training/gpt2-imdb-pos-v2/tokenizer_config.json',\n",
       " '/home/aicoder/training/gpt2-imdb-pos-v2/special_tokens_map.json',\n",
       " '/home/aicoder/training/gpt2-imdb-pos-v2/vocab.json',\n",
       " '/home/aicoder/training/gpt2-imdb-pos-v2/merges.txt',\n",
       " '/home/aicoder/training/gpt2-imdb-pos-v2/added_tokens.json',\n",
       " '/home/aicoder/training/gpt2-imdb-pos-v2/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"/home/aicoder/training/gpt2-imdb-pos-v2\", push_to_hub=False)\n",
    "tokenizer.save_pretrained(\"/home/aicoder/training/gpt2-imdb-pos-v2\", push_to_hub=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883ec8f-05e5-4e9b-a550-4b997c4939ab",
   "metadata": {},
   "source": [
    "**best-of-n sampler** class that serves as an alternative method of generating better model output. As to how it fares against the RL based fine-tuning, please look in the examples directory for a comparison example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fad8f9-4804-4200-870a-cd6e7f64b5ac",
   "metadata": {},
   "source": [
    "instantiate an instance of the class with a model, a length sampler, a tokenizer and a callable that serves as a **proxy reward pipeline** that outputs reward scores for input queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e4016-bc15-4231-809d-0fb64d2e4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from trl.extras import BestOfNSampler\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(min_length= -1, top_k=0.0, top_p= 1.0, do_sample= True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)\n",
    "\n",
    "reward_pipe = pipeline(\"sentiment-analysis\", model=reward_model, device=device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ref_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# callable that takes a list of raw text and returns a list of corresponding reward scores\n",
    "def queries_to_scores(list_of_strings):\n",
    "  return [output[\"score\"] for output in reward_pipe(list_of_strings)]\n",
    "\n",
    "best_of_n = BestOfNSampler(model, tokenizer, queries_to_scores, length_sampler=output_length_sampler,\n",
    "                          n_candidates=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df1fde-299b-4eb9-aabd-305a4330d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_of_n.generate(query_tensors, device=device, **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c416d-6536-4bd5-bfa4-08922f3b433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_of_n = BestOfNSampler(model,\n",
    "                           tokenizer,\n",
    "                           queries_to_scores,\n",
    "                           length_sampler=output_length_sampler,\n",
    "                           generation_config=generation_config)\n",
    "\n",
    "best_of_n.generate(query_tensors, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09462c93-282d-4b9b-99fa-f71bbf433c11",
   "metadata": {},
   "source": [
    "First step as always is to train your SFT model, to ensure the data we train on is in-distribution for the DPO (Direct Preference Optimizer) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188ba41-c2b0-4b01-a1f2-b715ff917ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe676d5-1220-4721-acbc-737a4d0e32f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
